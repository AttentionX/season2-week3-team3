[
    "Fine-Tuning can Distort Pretrained Features and Underperform\nOut-of-Distribution\nAnanya Kumar Aditi Raghunathan Robbie Jones\nTengyu Ma Percy Liang\nStanford University\nDepartment of Computer Science\n{ananya, aditir, rmjones, tengyuma, pliang}@cs.stanford.edu\nAbstract\nWhen transferring a pretrained model to a downstream task, two popular methods are full \ufb01ne-tuning\n(updating all the model parameters) and linear probing (updating only the last linear layer\u2014the \u201chead\u201d).\nIt is well known that \ufb01ne-tuning leads to better accuracy in-distribution (ID). However, in this paper, we\n\ufb01nd that \ufb01ne-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when\nthe pretrained features are good and the distribution shift is large. On 10 distribution shift datasets\n(Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \u2192STL, CIFAR10.1, FMoW, ImageNetV2,\nImageNet-R, ImageNet-A, ImageNet-Sketch), \ufb01ne-tuning obtains on average 2% higher accuracy ID but\n7% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and\nOOD accuracy arises even in a simple setting: \ufb01ne-tuning overparameterized two-layer linear networks.\nWe prove that the OOD error of \ufb01ne-tuning is high when we initialize with a \ufb01xed or random head\u2014this\nis because while \ufb01ne-tuning learns the head, the lower layers of the neural network change simultaneously\nand distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing\nthen full \ufb01ne-tuning (LP-FT), sometimes used as a \ufb01ne-tuning heuristic, combines the bene\ufb01ts of both\n\ufb01ne-tuning and linear probing. Empirically, LP-FT outperforms both \ufb01ne-tuning and linear probing on\nthe above datasets (1% better ID, 10% better OOD than full \ufb01ne-tuning).\n1 Introduction\nPretraining a model on a large dataset before transferring to a downstream task\u2019s training data substantially\nimproves accuracy over training from scratch\u2014for example, pretraining a ResNet-50 on unlabeled ImageNet\nboosts accuracy on CIFAR-10 from 94% to 98% (Chen et al., 2020a,b). Achieving high in-distribution\naccuracy is not enough: high-stakes applications such as poverty mapping in under-resourced countries (Jean\net al., 2016), self-driving cars (Yu et al., 2020), and medical diagnosis (AlBadawy et al., 2018), require\nmodels that also generalize to circumstances not seen in the training distribution. In addition to testing on\ndata drawn from the downstream task\u2019s training distribution (in-distribution; ID), it is increasingly important\nto test on data distributions unseen during training (out-of-distribution; OOD). OOD accuracy can be much\nlower than ID accuracy; for example, an ImageNet pretrained ResNet-50 \ufb01ne-tuned on CIFAR-10 gets 98%\naccuracy on CIFAR-10 (ID) but 82% on STL (OOD).\nAfter initializing with a pretrained model, two popular transfer methods are \ufb01ne-tuning (running gradient\ndescent on all the model parameters), and linear probing (tuning the head but freezing lower layers). In the ID\nsetting, it is well known that \ufb01ne-tuning leads to better accuracy than linear probing (Kornblith et al., 2019;\n1arXiv:2202.10054v1  [cs.LG]  21 Feb 2022",
    "Figure 1: Given a good feature extractor (top-left), a randomly initialized head is added to map features\nto outputs and we can (a) \ufb01ne-tune all the model parameters or (b) linear-probe, which freezes the feature\nextractor and trains only the head. We run experiments on ten distribution shifts. Fine-tuning does well when\nthe test example is sampled from the \ufb01ne-tuning distribution (ID), but can underperform on test examples\nsampled from OOD distributions (when the distribution shift is large). (c) Our theory indicates that \ufb01ne-tuning\ncan distort the pretrained feature extractor and lead to poor OOD accuracy, but initializing with a linear\nprobed head can \ufb01x this\u2014empirically LP-FT gets better accuracies both ID and OOD.\nZhai et al., 2020; He et al., 2020),1and even when testing OOD, prior work usually \ufb01ne-tunes all parameters\nof their model (Hendrycks et al., 2019a; Miller et al., 2021; Andreassen et al., 2021). Intuitively, \ufb01ne-tuning\nall layers of a network can improve pretrained features by adapting them to the speci\ufb01c task, while linear\nprobing simply inherits the frozen pretrained features.\nIn this work, we investigate the OOD accuracy of \ufb01ne-tuning and linear probing and \ufb01nd that surprisingly,\n\ufb01ne-tuning can do worse than linear probing in the presence of large distribution shift. We experiment\non ten distribution shift benchmarks (Breeds Living17, Breeds Entity30, DomainNet, CIFAR \u2192STL,\nCIFAR10.1, FMoW geo-shift, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), initializing with\ngood pretrained features from MoCo-v2 (Chen et al., 2020b) and CLIP (Radford et al., 2021). While both\nmethods offer gains over training from scratch, \ufb01ne-tuning improves the average ID accuracy relative to\nlinear probing from 83% to85% but brings down the OOD accuracy from 66% to59% (Figure 1).\nUnder what conditions does \ufb01ne-tuning underperform linear probing? We theoretically consider \ufb01ne-tuning a\ntwo-layer linear network in an overparameterized regression setting where the feature extractor layer has been\npretrained to map high-dimensional inputs to useful, lower-dimensional, features. We prove that \ufb01ne-tuning\nis worse than linear probing on directions outside the span of the training data when using \u201cgood\u201d pretrained\nfeatures. Even with an in\ufb01nitesimally small learning rate, \ufb01ne-tuning distorts pretrained features\u2014the features\nof ID training data are updated while those of OOD data change less. Since the head and feature extractor are\nsimultaneously optimized during \ufb01ne-tuning to a con\ufb01guration that works well on ID training data, the head\nonly accomodates the distorted features of ID points and performs poorly (relative to linear probing) on the\nless changed features of OOD points. Interestingly, we show that this feature distortion issue cannot be simply\n\ufb01xed by early stopping\u2014throughout the entire process of \ufb01ne-tuning, we never pass through parameters\nthat do well OOD (relative to linear probing). On the other hand, given \u201cgood\u201d features, linear-probing\n1Probing is commonly used but usually for interpretability or assessing feature quality.\n2\nFigure. The image depicts a number of graphs, each representing a different aspect of the training process. There is a bar chart at the top that shows the percentage of correct answers and an area chart showing the percent of incorrect answers. In addition, there is a pie chart in the lower right corner that shows the average score for each test. Additionally, there is a bar chart at the upper right corner that shows the percentage of correct answers and an area chart showing the average score for each test.\n",
    "extrapolates better OOD because it preserves pretrained features, but does not do as well as \ufb01ne-tuning ID\nbecause linear probing cannot adapt the features to the downstream task.\nTechnical challenges. Existing theoretical work on transfer learning focuses on linear probing (Wu et al.,\n2020; Tripuraneni et al., 2020; Du et al., 2020). In contrast, analyses of \ufb01ne-tuning is scarce and challenging\nbecause it requires understanding the training dynamics, instead of only the loss function and its global\nminimizers. In fact, \ufb01ne-tuning and training from scratch optimize the same training loss and only differ\nin their initializations (pretrained vs random). A mathematical analysis that distinguishes them needs\nto capture properties of the different minima that these algorithms converge to, a phenomenon that is\nsometimes theoretically referred to as the implicit regularization effect of initialization (Neyshabur et al.,\n2014). Accordingly, our analysis reasons about the parameters that gradient methods pass through starting\nfrom the pretrained initialization, which is challenging because this is a non-convex optimization problem\nand there is no known closed form for this trajectory. Two-layer linear networks are widely studied in the\nliterature on implicit regularization (Saxe et al., 2014; Gunasekar et al., 2017; Gidel et al., 2019; Arora et al.,\n2018). However, they analyze random and often small initializations, which don\u2019t capture pretraining.\nAlgorithmic implications. Our theory shows that \ufb01ne-tuning underpeforms because when trying to \ufb01t ID\ntraining data with a randomly initialized head, the feature extractor changes signi\ufb01cantly for ID examples,\nmaking features for ID and OOD examples largely inconsistent. This can be \ufb01xed by initializing with a good\nhead that does not need to be updated much during \ufb01ne-tuning, reducing how much the feature extractor\nchanges. This suggests a simple two-step strategy of \ufb01rst linear-probing to \ufb01nd a good head and then full\n\ufb01ne-tuning (LP-FT). Empirically, LP-FT outperforms \ufb01ne-tuning and linear-probing, both ID and OOD . Even\non CIFAR-10.1 (small distribution shift), where \ufb01ne-tuning is better for both ID and OOD, we \ufb01nd LP-FT\noutperforms \ufb01ne-tuning on both metrics. LP-FT and vanilla \ufb01ne-tuning use similar amounts of compute\nbecause the \ufb01rst step of linear probing is relatively very cheap. Prior work has used LP-FT (Levine et al.,\n2016; Kanavati & Tsuneki, 2021) (or variants such as layerwise \ufb01ne-tuning (Howard & Ruder, 2018) or larger\nlearning rates for the head layer (Prabhu et al., 2021))\u2014however it has not been used for robustness / OOD\naccuracy, and we show that it addresses the ID-OOD tradeoff theoretically and empirically. Note that LP-FT\nis not meant to be a SOTA method but a simple, principled way to get good ID and OOD accuracy\u2014we hope\nour analysis inspires even better methods for robust \ufb01ne-tuning.\nEmpirical validation. Finally, we check whether \ufb01ne-tuning underperforms and LP-FT works, for the\nreasons predicted by our feature distortion theory. As predicted by the theory, we \ufb01nd that: (1) \ufb01ne-tuning\nindeed never matches the OOD accuracy of linear probing throughout the course of training (if the pretrained\nfeatures are good, and OOD shift is large); (2) \ufb01ne-tuning changes the features for ID examples more than for\nOOD examples, leading to distortions; (3) LP-FT indeed changes both ID and OOD features 10\u00d7\u2212100\u00d7\nless than \ufb01ne-tuning does; (4) \ufb01ne-tuning can do better than linear probing OOD if the pretrained features\nare not very high quality (MoCo-v1 instead of MoCo-v2) or the ID and OOD datasets are very close (e.g.,\nCIFAR-10 and CIFAR-10.1); and (5) LP-FT gets the best of both worlds, better accuracies than \ufb01ne-tuning\nand linear probing, both ID and OOD (Figure 1).\n2 Setup\nTask and evaluation. Given training examples sampled from some distribution Pid, our goal is to learn a\npredictorf:Rd\u2192Y to map inputs x\u2208Rdto outputsy\u2208Y. We evaluate predictors on their standard\n\u201cin-distribution\u201d (ID) performance Lidon new test samples drawn from Pidthat the training data is also\n3",
    "sampled from. We also evaluate classi\ufb01ers on their \u201cout-of-distribution\u201d (OOD) performance Loodon test\nsamples drawn from a new distribution Poodthat is different from Pid. Formally, for some loss function \u2113, we\nevaluate classi\ufb01ers on:\nLid(f) = E\n(x,y)\u223cPid[\u2113(f(x),y)]andLood(f) = E\n(x,y)\u223cPood[\u2113(f(x),y)]. (2.1)\nModels. In this work, we focus on predictors that leverage pretrained representations. We parameterize the\n\ufb01nal predictor fas follows: given features gB(x)\u2208Rkfor some feature extractor parameters B\u2208B, and a\nlinear \u201chead\u201d v\u2208V, we havefv,B(x) =v\u22a4gB(x). In our experiments (Section 4), gBis a deep network and\nin our theory (Section 3), gBis a linear projection.\nWe assume access to some initial pretrained feature extractor B0that is obtained by training on potentially\nlarge amounts of data from a distribution that contains unlabeled or weakly supervised xinputs from Pid\nandPood. We focus on two popular methods to learn a predictor fv,Bgiven training data from Pid: (i) linear\nprobing where B=B0and the linear head is obtained by minimizing some loss (e.g., logistic loss for\nclassi\ufb01cation, squared loss for regression) on the training data, and (ii) \ufb01ne-tuning where both vandBare\nupdated by performing gradient descent on some loss on the training data with Binitialized at B0.\n3 Theory: \ufb01ne-tuning distorts pretrained features\nOur goal is to understand under what conditions \ufb01ne-tuning does worse than linear probing out-of-distribution\n(OOD).2We consider a linear setting (feature extractor gBis linear) where the pretrained features are \u201cgood\u201d\nand the OOD shift is large (Section 3.1). We prove our main result: that \ufb01ne-tuning, in which all model\nparameters are updated, distorts features and gets suboptimal OOD error (Section 3.2, Theorem 3.3). We use\nthis result to show that linear probing gets better OOD error but worse ID error than \ufb01ne-tuning (Section 3.3).\nFinally, we explain why linear probing then \ufb01ne-tuning can mitigate this ID-OOD tradeoff (Section 3.4).\nOur analysis handles two key challenges which distinguishes it from prior work on transfer learning in linear\nmodels (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020; Xie et al., 2021a). Prior work focuses on\nlinear probing, while we study \ufb01ne-tuning where the resulting optimization problem is non-convex . We also\nstudy overparameterized models where the training loss alone does not determine test performance\u2014this\ncaptures the fact that both training neural networks from scratch and \ufb01ne-tuning them have the same training\nloss but very different test performance. However, it also makes the analysis challenging because we need to\nreason about the trajectory of gradient methods starting from a pretrained initialization, which has no known\nclosed form.\n3.1 Linear overparameterized setting\nFor our analysis, we focus on regression, where Y=Rand\u2113(\u02c6y,y) = (\u02c6y\u2212y)2is the squared loss.\nModels. Recall from Section 2 that we parameterize predictors in terms of feature extractor and head\nparameters. In this section, we study models where the feature extractor is linear, i.e. fv,B(x) =v\u22a4Bx\nwhereB\u2208B=Rk\u00d7d, andv\u2208V=Rk.\n2For example, without additional assumptions we can have Pid=Poodand so the same method will do better both ID and OOD.\n4",
    "Good pretrained features. For simplicity, we assume the models are well-speci\ufb01ed i.e. y=v\u22a4\n\u22c6B\u22c6xwhere\nv\u22c6\u2208RkandB\u22c6\u2208Rk\u00d7d.3Note thatB\u22c6andv\u22c6are only unique up to rotations, i.e., for any rotation\nmatrixU,(Uv\u22c6)T(UB\u22c6)x=vT\n\u22c6B\u22c6x. As in prior work (Tripuraneni et al., 2020) suppose B\u22c6,B0have been\northogonalized to have orthonormal rows. Suppose we have a pretrained feature extractor B0close toB\u22c6, so\nd(B0,B\u22c6)\u2264\u03f5where the distance dis de\ufb01ned below:\nDe\ufb01nition 3.1 (Feature Extractor Distance) .The distance between feature extractors B,B\u2032\u2208Rk\u00d7d(with\northonormal rows) is given by (where the min is over rotation matrices U\u2208Rk\u00d7k):\nd(B,B\u2032) = min\nU\u2225B\u2212UB\u2032\u22252, (3.1)\nPretraining coverage intuition : Intuitively, the existence of B\u22c6corresponds to assuming that there exists a\nshared set of useful features for ID ( Pid) and OOD ( Pood). We also assume that B0is close toB\u22c6\u2014one way\nthis can happen is if pretraining is done on large scale data and has seen unlabeled or weakly supervised x\ninputs that cover the support of PidandPood. Formally, the task diversity assumption in Tripuraneni et al.\n(2020) is suf\ufb01cient (but not necessary) for obtaining a good B0. In our paper we show that even if we have\nthese good features, \ufb01ne-tuning can distort them and lead to low OOD accuracy.\nTraining data. LetX\u2208Rn\u00d7d,X\u0338= 0be a matrix encoding ntraining examples from Pidwhere each of\nthenrows is a training input. Let Y\u2208Rnbe the corresponding outputs. Let S=rowspace (X)be the\nm-dimensional subspace spanning the training examples. We consider an overparameterized setting where\n1\u2264m<d\u2212k. Intuitively, the input dimension dis high (e.g., 10K), feature dimension kis lower (e.g.,\n100) andmis in the middle (e.g., 5K).4\nLarge OOD shift. We assume that the OOD data contains examples outside the span of the training data.\nFormally, let Poodhave second moment \u03a3 =E[xx\u22a4]wherex\u223cPood, and we assume \u03a3is invertible.56\nTraining methods. Given training data and a pretrained feature extractor B0, we study the two popular\nmethods of linear probing (LP) and \ufb01ne-tuning (FT) to learn the \ufb01nal predictor. Both methods involve\noptimizing the training loss via gradient descent (or variants). In order to effectively analyze these gradient\nbased algorithms, we study vanishing step sizes leading to gradient \ufb02ows. Gradient \ufb02ows can be thought of\nas a continuous time analogue of gradient based methods and have been extensively studied in recent years as\na way to understand gradient based methods (Gunasekar et al., 2017; Arora et al., 2018; Du et al., 2018).\nFormally, for training loss \u02c6L(v,B) =\u2225XB\u22a4v\u2212Y\u22252\n2, the gradient \ufb02ow differential equations for LP and FT\n3We note that our main contribution\u2014analysis of \ufb01ne-tuning (Theorem 3.3)\u2014does not require this well-speci\ufb01ed assumption.\nWe compare \ufb01ne-tuning with linear probing by adapting earlier work on linear probing which requires well-speci\ufb01cation.\n4Indeed, in neural tangent kernel approximations, the input dimension dis the number of weights in a neural network which\nis much larger than the span of the training data m, while the feature dimension kof neural networks is usually smaller than m.\nExtending our results to the NTK regime could be an interesting future direction.\n5We don\u2019t need \u03a3to be invertible, but just require the OOD span T=Range (\u03a3)to have some directions outside the training\nspan: dim(T\\S)> k.\n6Prior work on distribution shift (Rosenfeld et al., 2021; Kamath et al., 2021; Chen et al., 2021b) often considers a worst case loss\nover some set\u2014we can equivalently write Loodas a worst case loss over distributions (equivalently, individual points) of bounded\nnorm: max x(v\u22a4\n\u22c6B\u22c6x\u2212v\u22a4Bx)2overx\u22a4\u03a3\u22121x\u22641. If\u03a3 =Idthen this is just the worst case loss over \u2225x\u22252\u22641.\n5",
    "Figure 2: A toy version of our theory illustrating why \ufb01ne-tuning distorts features, with inputs in 2D. Given\ninputx, the ground truth output is y=w\u22a4\n\u22c6x. The ID data is along the x-axis and the pretrained feature\nextractor isB0. (a) Linear probing learns wlp, a scaling of the pretrained feature extractor that gets the ID\ndata correct ( wlpandw\u22c6have the same xcoordinate as indicated by the vertical dotted line). (b) Fine-tuning\nupdates the pretrained feature extractor along the ID data (so horizontally) to get Bft, and then learns a scaling\nof these features that gets the ID data correct. While both methods get ID data correct, \ufb01ne-tuning makes\nlarge errors perpendicular to the ID data, because \ufb01ne-tuning updates B0along the ID direction but not the\nperpendicular direction (we call this feature \u201cdistortion\u201d).\nare as follows:\n\u2202tvft(t) =\u2212\u2207v\u02c6L(vft(t),Bft(t)), \u2202tBft(t) =\u2212\u2207B\u02c6L(vft(t),Bft(t)), (3.2)\n\u2202tvlp(t) =\u2212\u2207v\u02c6L(vlp(t),B0), \u2202tBlp(t) = 0, (3.3)\ninitialized with Bft(0) =Blp(0) =B0andvft(0) =vlp(0) =v0. In practice, the head parameter v0is\ninitialized randomly\u2014our results hold for any standard random initialization (Glorot & Bengio, 2010), for\nexamplev0\u223cN(0,\u03c32I)for any\u03c32, or zero initialization where v0= 0. Recall that the initial value of the\nfeature extractor B0is obtained via pretraining.\nThe \ufb01nal LP and FT solutions are the limit points of the corresponding gradient \ufb02ows:\nv\u221e\nft= lim\nt\u2192\u221evft(t)andB\u221e\nft= lim\nt\u2192\u221eBft(t), (3.4)\nv\u221e\nlp= lim\nt\u2192\u221evlp(t)andB\u221e\nlp= lim\nt\u2192\u221eBlp(t) =B0. (3.5)\n3.2 Fine-tuning distorts pretrained features\nThe more common method of using a pretrained feature extractor is \ufb01ne-tuning (FT) which typically improves\nID performance relative to linear probing (LP). In this section, we show theoretically that FT can distort\nfeatures leading to poor OOD performance. We \ufb01rst present the key intuitions demonstrating potential issues\nof FT and then present our formal theorem lower bounding the OOD error of FT (Section 3.2.2).\n3.2.1 Key intuitions\nThere are two main observations that we use to characterize when and why FT has higher OOD error than\nlinear probing.\n6\nFigure. The image depicts two different diagrams, one showing a general heating curve and the other showing a specific heating curve. Both of the diagrams are located in the center of the image, with the central diagram being closer to the viewer. There is also a smaller diagram on the right side of the image, which can be seen from a distance.\n",
    "1. Features get distorted: representations change only in the ID subspace (i.e., subspace spanned by the\ntraining data) and are unchanged in the orthogonal subspace. To see this, we take the derivative of the\ntraining loss\u02c6L(v,B) =\u2225XB\u22a4v\u2212Y\u22252\n2with respect to the feature extractor parameter B:\n\u2207B\u02c6L(v,B) = 2v(Y\u2212XBv )\u22a4X. (3.6)\nBy de\ufb01nition, if uis a direction orthogonal to the training subspace S=rowspace (X), then\u2207B\u02c6L(v,B)u=\n0, that is the gradient updates to Bdo not modify Buforu\u2208S\u22a5. However, the gradient is non-zero for\ndirectionsuin the ID subspace and the corresponding features Buchange across the \ufb01ne-tuning process. We\ncall this feature distortion: the features in some directions are changed but not others. Next, we explain why\nthis can lead to high OOD error.\n2. Distorted features can lead to higher OOD error. Consider a toy example (Figure 2) where d= 2\nand the dimensionality of the representations k= 1. The linear head vis a scalar quantity that denotes\nhow much the feature extractor Bhas to be scaled by. Suppose the ID-subspace is the x-axis. There are\ndifferent ways of \ufb01tting the ID subspace depending on the feature extractors Bas shown in the Figure\u2014both\n\ufb01ne-tuned and linear probed estimators match the true parameter in the ID subspace (since wlp,wft,w\u22c6have\nthe same projection on the x-axis). If the feature extractor were optimal or scaled versions of the optimal,\ngood performance on the ID subspace would translate to good performance everywhere, even in directions\northogonal to the ID subspace. However, in FT, the features change only for inputs in the ID subspace (see\n(1)) and thus the updated features are notsimply scaled but distorted. In Figure 2, this corresponds to the\nfeature extractor B0changing along the x-axis. In this case even if the ID error is low, error in directions\northogonal to the ID subspace can be high, leading to high OOD error.\nThe only way the pretrained features are not distorted and only scaled during FT is if the initial feature\nextractorB0is exactly aligned with the ID subspace. In Figure 2, if B0is along thex-axis (the ID subspace),\nthen updating the features exclusively along the x-axis would simply scale the initial features. In this\ncase linear probing and \ufb01ne-tuning will have identical behavior. If the angle between B0and thex-axis is\nnon-zero\u2014which occurs with probability 1if the training data Xor pretrained feature extractor B0involves\neven a tiny amount of randomness e.g., from SGD in pretraining\u2014the updates would lead to distortions. In\nhigh dimensions, we measure the alignment between B0and the ID subspace with the largest principal angle:\nDe\ufb01nition 3.2 (largest principal angle) .LetAandBbe arbitrary subspaces, and EandFbe matri-\nces with orthonormal columns than span AandBrespectively, with r= min(dim( A),dim(B)). Then\ncos\u03b8max(A,B) =\u03c3r(E\u22a4F), which is the r-th largest singular value of E\u22a4F.\nNote thatE,F are not unique in De\ufb01nition 3.2, but \u03c3r(E\u22a4F)is the same for every valid choice of EandF.\nSee Appendix A.1 for more information on principal angles.\n3.2.2 General result on the OOD error of \ufb01ne-tuning\nOur main theorem lower bounds the OOD error of \ufb01ne-tuning outside the span of the training data. In\nSection 3.3 we compare this lower bound with an upper bound on the OOD error of linear probing.\nTheorem 3.3. In the overparameterized linear setting, let S\u22a5=rowspace (X)\u22a5,R0=rowspace (B0), and\nv\u22c6,B\u22c6be the optimal parameters with w\u22c6=B\u22c6v\u22c6. Ifcos\u03b8max(R0,S\u22a5)>0, then for all time steps t, the\n7",
    "OOD error of the \ufb01ne-tuning iterates (Bft(t),vft(t))is lower bounded:\n\u221a\nLood(vft(t),Bft(t))\u2265\u221a\n\u03c3min(\u03a3)(cos\u03b8max(R0,S\u22a5)\u221a\nkmin(\u03d5,\u03d52/\u2225w\u22c6\u22252)\n(1 +\u2225w\u22c6\u22252)2\u2212\u03f5)\n, (3.7)\nwhere\u03d52=|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|is de\ufb01ned to be inital head alignment error and \u03f5\u2265d(B0,B\u22c6)is the error\nin the pretrained feature extractor.\nProof sketch. Since the features do not change for examples in S\u22a5(perpendicular to the training data), we\nshow that in order to achieve low error on S\u22a5the linear head vft(t)would have to become very similar to\nthe optimal v\u22c6at some time t. The head initialization v0is random (or zero) and likely to be far from v\u22c6\n(measured by the alignment error \u03d5), so the head would have to change a lot to get close to v\u22c6. As we see from\nthe \ufb01ne-tuning gradient \ufb02ow (3.2), vft(t)andBft(t)change in a \u201ccoupled\u201d manner, and a \u201c\u2019balancedness\u201d\ninvariant in Du et al. (2018) holds across the \ufb01ne-tuning trajectory. Correspondingly, if vft(t)changes a lot\nand gets close to v\u22c6, the features Bft(t)also change a lot for examples in S\u2014we show that this would lead to\nhigh error on examples in S. Either way, \ufb01ne-tuning would get some subspace ( SorS\u22a5) of examples wrong,\nleading to high OOD error. The full proof appears in Appendix A.\nInterpretations of various quantities. Quality of pretrained features ( \u03f5).To unpack the bound consider a\nspecial case where the pretrained features are perfect ( \u03f5= 0). With perfect features, Proposition A.20 shows\nthat linear probing gets zero OOD error. Theorem 3.3 shows that Lood(vft(t),Bft(t))>0at all timest\u2014so\n\ufb01ne-tuning underperforms when the features are perfect. The \u03f5>0case just captures the fact that even if\nthe features are not perfect, \ufb01ne-tuning can still get positive error. Ideally we would like the lower bound to\nincrease if we have worse features (so \u201c +\u03f5\u201d instead of \u201c\u2212\u03f5\u201d in the bound)\u2014the reason we do not is that the\nerrors of the pretrained feature extractor d(B0,B\u22c6)and the \ufb01ne-tuning step can potentially cancel out.7\nAlignment error of random head initialization ( \u03d52).The lower bound (Equation A.14) increases as \u03d52\nincreases i.e. alignment error increases because the gradient updates to the head and feature extractor are\ncoupled. If the head were somehow initialized perfectly at v\u22c6, then \ufb01ne-tuning updates may not increase\nthe OOD error. However, when the head is randomly initialized (or initialized to zero) as is standard in\n\ufb01ne-tuning, the alignment error is high, leading to high OOD error. We use this insight in Section 3.4 to show\nthat better head initialization (namely via linear probing) improves OOD performance of \ufb01ne-tuning.\nSpan of Training data ( S).Theorem 3.3 lower bounds the error outside the span of the training data. If the\ntraining dataset is very small, then even the support of the ID distribution Pidmay not be spanned by the\ntraining data, and the ID error can be large. Indeed, even in the ID setting Kornblith et al. (2019) show that\nlinear probing can do better than \ufb01ne-tuning if we have very few training examples, but \ufb01ne-tuning does\nbetter on all 11 of their datasets once we have more than just 30 examples per class.\nConjectures for improved bounds. We believe it may be possible to improve cos\u03b8max(R0,S\u22a5)to the cosine\nof the minimum principal angle.8This may look like a technicality but would be a substantial improvement,\nbecause it would imply that \ufb01ne-tuning has error in every direction outside the training span, whereas we\nshow that it would have errors in some directions. Our proof strategy requires the maximum principal angle\n(a crucial step is a variational characterization of the maximal principal angle in Lemma A.2\u2014we use this in\nStep 1 of the proof in Appendix A to show that to get low OOD error vft(t)must become similar to v\u22c6).\n7Intuitively this cancelation is very \u201cunlikely\u201d to happen, and we hope future work can capture this intuition.\n8Which would be a larger/better lower bound since the cosine of a smaller quantity is larger.\n8",
    "3.3 Linear probing vs. \ufb01ne-tuning\nIn this section, we use our main theorem on \ufb01ne-tuning (Theorem 3.3) and adapt prior work on linear probing\nto show that linear probing is better than \ufb01ne-tuning OOD, but worse ID, when the ID distribution has density\non a lowerm<d dimensional subspace S, andB0is close toB\u22c6(so we have \u201cgood\u201d pretrained features).\nAssumption 3.4 (ID subspace assumption) .We assume that the ID data lies on an m-dimensional subspace\nSwherek<m<d\u2212k, and we have n\u2265mtraining examples. Formally, let Pzbe a distribution on Rm\nwhich has density, and let the columns of F\u2208Rd\u00d7mform an orthonormal basis for S. ThenPidhas the\ndistribution of Fzwherez\u223cPz.\nRecall that the ID error is the expected mean-squared error over the ID distribution Pid:\nLid(v,B) =E\nx\u223cPid[(v\u22a4\n\u22c6B\u22c6x\u2212v\u22a4Bx)2] (3.8)\nOOD comparison : Under mild non-degeneracy conditions, we show that as the feature extractor error \u03f5\ngoes to 0, linear probing does much better than \ufb01ne-tuning OOD: the ratio of the losses goes to 0. The non-\ndegeneracy conditions are similar to Section 3.2\u2014we require that the training data cannot be exactly in the\nsame direction or orthogonal to the pretrained features, formally that cos\u03b8max(R\u2217,S)andcos\u03b8max(R\u2217,S\u22a5)\nare not 0whereR\u2217=rowspace (B\u22c6). In the toy example in Figure 2, this means that xidcannot be exactly in\nthe same direction or orthogonal to B\u22a4\n0\u2014in these cases \ufb01ne-tuning and linear probing get the same loss but\nin all other cases in the toy example in Figure 2 linear probing does better OOD.\nTheorem 3.5 (Informal version of Theorem A.8) .In the linear overparameterized setting, under the ID\nsubspace assumption (Assumption 3.4), if cos\u03b8max(R\u2217,S)\u0338= 0 andcos\u03b8max(R\u2217,S\u22a5)\u0338= 0 whereR\u2217=\nrowspace (B\u22c6), then,\nLood(v\u221e\nlp,B0)\nLood(vft(t),Bft(t))p\u21920,asB0\u2192B\u22c6. (3.9)\nThis holds for all times tfor FT (and therefore also for the limit v\u221e\nft,B\u221e\nft) and the LP iterates converge to\nv\u221e\nlp,B0as a result of the gradient \ufb02ow on a convex problem.\nIntuitively, if the pretrained features are good, LP learns a near optimal linear head which has small\nOOD error (Lemma A.14) but \ufb01ne-tuning has high OOD error (Theorem 3.3). We give a more formal\nversion of Theorem 3.5 and a proof in Appendix A.3. If Pzis isotropic Gaussian, we can get a better\nresult: Theorem A.15 derives a threshold T(in terms of d,n,k ) where LP does better than FT if \u03f5 < T ,\ninstead of just the asymptotic result ( B0\u2192B\u22c6). Theorem 3.5 requires that cos\u03b8max(R\u2217,S)\u0338= 0 and\ncos\u03b8max(R\u2217,S\u22a5)\u0338= 0\u2014intuitively, for any subspace a small perturbation would make these angles non-\nzero and the assumption would hold. To illustrate that these assumptions typically hold, Lemma A.16 in\nAppendix A proves that if Sis a random m-dimensional subspace then these angles are non-zero almost\nsurely.\nID comparison : When the pretrained features have some error, we show that \ufb01ne-tuning does better than\nlinear probing ID because \ufb01ne-tuning can update the features to \ufb01t the ID data.\nIf the pretrained features are perfect so that the optimal predictor can be written as a linear combination of the\npretrained features ( w\u22c6=B\u22a4\n\u22c6v\u22c6\u2208rowspace (B0)), then both linear probing and \ufb01ne-tuning get zero ID error.\nHowever, if the pretrained representation has some error, and the training data satis\ufb01es a mild non-degeneracy\n9",
    "condition, then LP has high ID error because there is no linear head on B0that \ufb01ts the training data perfectly.\nFT, on the other hand, can update the features to \ufb01nd a new B\u221e\nftthat can \ufb01t the training data perfectly with a\nlinear headv\u221e\nft.\nThe non-degeneracy condition is similar to our previous results, and holds with probability 1 if the ID\nsubspace is chosen randomly, from Lemma A.16. Formally, let Raugbe ak+ 1dimensional subspace\nspanningR0\u222a{w\u22c6}, where we recall that R0=rowspace (B0). Then we just require that the ID subspace S\nandRaugare not orthogonal: cos\u03b8max(S,R aug)\u0338= 0. We state the formal proposition below and give a proof\nin Appendix A.\nProposition 3.6. In the linear overparameterized setting, under the ID subspace assumption (Assumption 3.4),\nletR0=rowspace (B0), andRaug=Span({w\u22c6}\u222aR0). Supposew\u22c6\u0338\u2208R0,cos\u03b8max(S,R aug)\u0338= 0, and\nthat \ufb01ne-tuning converges to a local minimum of its loss, then \ufb01ne-tuning does better ID almost surely:\nLid(v\u221e\nft,B\u221e\nft)<L id(v\u221e\nlp,B0)with probability 1 (over the randomness of the training examples).\nTo summarize, we proved that there are tradeoffs between ID and OOD error: FT has lower ID error but\nhigher OOD error than LP. In the next section, we extend our theoretical insights to illustrate why a simple\nvariant of FT may mitigate such tradeoffs.\n3.4 Linear probing then \ufb01ne-tuning: a simple variant to mitigate tradeoffs\nThe advantage of \ufb01ne-tuning is it can adapt both the feature extractor and head to \ufb01t the downstream task.\nCan we keep this bene\ufb01t while ensuring that our OOD error is low when we have good pretrained features?\nGoing back to Theorem 3.3, we see that the alignment error in the head initialization \u03d52= (v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2\nplays an important role. The issue with FT was that under random or zero initialization, \u03d52is usually large\nand since the gradient updates to the feature extractor parameter are coupled with that of the head parameter,\nthe features get distorted in a manner that increases the OOD error. This suggests that we should use a better\nhead initialization\u2014one obtained from linear probing. If the pretrained features are decent, a linear probed\nhead would be much better aligned with v\u22c6allowing the features to be updated in a manner that does not\nincrease the OOD error much.\nWe formally prove this intuition in a simple setting where we have perfect pretrained features. Of course, if\nwe have perfect pretrained features, linear probing alone gets zero OOD error\u2014so Proposition 3.7 is just a\n\ufb01rst cut result to illustrate that if initialized well, full \ufb01ne-tuning does not distort features.\nProposition 3.7. Suppose we have perfect pretrained features B0=UB\u22c6for some rotation U. LetR0=\nrowspace (B0). Under the non-degeneracy conditions cos\u03b8max(R0,S)\u0338= 0,cos\u03b8max(R0,S\u22a5)\u0338= 0:\n\u2200t,L ood(Bft(t)\u22a4vft(t))>0,ifv0\u223cN(0,\u03c32I)is randomly initialized (FT) , (3.10)\n\u2200t,L ood(Bft(t)\u22a4vft(t)) = 0,ifv0is initialized to v\u221e\nlp(LP-FT). (3.11)\nThe case where we do not have perfect features ( d(B0,B\u22c6)>0) is challenging to analyze because except\nin very special cases, there is no closed form for the \ufb01ne-tuning iterates (vft(t),Bft(t)). Our proof of\nTheorem 3.3 leveraged invariants to show a lower bound on the error of \ufb01ne-tuning when v0andv\u22c6are\ndifferent, but we were not able to show an upper bound .\n10",
    "4 Experiments\nWe run experiments on ten distribution shifts to see if our theoretical predictions on the relative performance\nof linear probing (LP), \ufb01ne-tuning (FT), and LP-FT, generalize to deep neural networks on real datasets. As\nexpected, given good pretrained features, \ufb01ne-tuning (FT) does better ID but worse on large OOD shifts than\nlinear probing (LP). In particular, ID and OOD accuracy are not correlated, unlike Recht et al. (2018) but\nlike Xie et al. (2021a). As predicted by the theory, we \ufb01nd that LP-FT does better than both methods ID and\nOOD and gets around this tradeoff. Our theory also predicts that the reason for these trends is that \ufb01ne-tuning\ndistorts features, and we see that this distortion indeed happens in practice. For more details on datasets,\npretraining models, and experiment protocols, see Appendix B. The datasets we use are:\n\u2022DomainNet (Peng et al., 2019) is a standard domain adaptation dataset. Here, our ID dataset contains\n\u201csketch\u201d images (e.g., drawings of apples, elephants, etc), and the OOD dataset contains \u201creal\u201d, \u201cclipart\u201d,\nand \u201cpainting\u201d images of the same categories. We use the version of the dataset from Tan et al. (2020).\n\u2022Living-17 andEntity-30 are sub-population shift datasets from the BREEDS benchmark (Santurkar et al.,\n2020). In Living-17 the goal is to classify an image as one of 17 animal categories such as \u201cbear\u201d\u2014for\nexample, the ID dataset contains images of black bears and sloth bears and the OOD dataset has images\nof brown bears and polar bears. In Entity-30 the goal is to classify an image as one of 30 entities such as\n\u201cfruit\u201d or \u201cinsect\u201d.\n\u2022FMoW Geo-shift is adapted from the satellite remote sensing dataset Functional Map of the World (Christie\net al., 2018; Koh et al., 2021). The goal is to classify a satellite image into one of 62 categories such as\n\u201cimpoverished settlement\u201d or \u201chospital\u201d. Our ID dataset contains images from North America, and the\nOOD dataset contains images from Africa and Europe.\n\u2022CIFAR-10\u2192STL is a standard domain adaptation dataset (French et al., 2018), where the ID is CIFAR-\n10 (Krizhevsky, 2009), and the OOD is STL (Coates et al., 2011). The task is to classify an image into one\nof 10 categories such as \u201cdog\u201d, \u201ccat\u201d, or \u201cairplane\u201d\u2014as usual, we remove the \u201cmonkey\u201d class in STL since\nCIFAR-10 has no \u201cmonkey\u201d images.\n\u2022CIFAR-10\u2192CIFAR-10.1 (Recht et al., 2018) is a dataset collected using a very similar protocol to\nCIFAR-10, and the authors describe it as \u201ca minute distributional shift\u201d. The hope is that a classi\ufb01er trained\non CIFAR-10 gets high accuracy on CIFAR-10.1.\n\u2022ImageNet -1K (Russakovsky et al., 2015) is a large scale dataset containing over a million images, where\nthe goal is to classify an image into one of 1000 categories such as \u201cYorkshire terrier\u201d, \u201cLabrador retriever\u201d,\n\u201cacoustic guitar\u201d, \u201clibrary\u201d, \u201cschool bus\u201d, etc. We \ufb01ne-tune on ImageNet as the ID dataset, and evaluate\non four standard OOD datasets: ImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020),\nImageNet-A (Hendrycks et al., 2019b), and ImageNet-Sketch (Wang et al., 2019).\nPretraining and models. We use a CLIP pretrained ViT-B/16 for ImageNet. For the other datasets we use a\nResNet-50 architecture and consider a diverse range of pretraining methods and datasets: MoCo-v2 (Chen\net al., 2020b), CLIP (Radford et al., 2021), and MoCo-TP (Ayush et al., 2020). In Appendix B, we also show\nresults for a CLIP-ViT-B/16 and more \ufb01ne-tuning baselines on Living-17.\n11",
    "Table 1: ID accuracies with 90% con\ufb01dence intervals over 3 runs\u2014\ufb01ne-tuning does better than linear\nprobing on all datasets except DomainNet (which could be because the version of the DomainNet training\ndataset from Tan et al. (2020) is fairly small, with around 20K examples). LP-FT does the best on all except\nFMoW where it is in between linear probing and \ufb01ne-tuning.\nCIFAR-10 Ent-30 Liv-17 DomainNet FMoW ImageNet Average\nFT 97.3 (0.2) 93.6 (0.2) 97.1 (0.2) 84.5 (0.6) 56.5 (0.3) 81.7 (-) 85.1\nLP 91.8 (0.0) 90.6 (0.2) 96.5 (0.2) 89.4 (0.1) 49.1 (0.0) 79.7 (-) 82.9\nLP-FT 97.5 (0.1) 93.7 (0.1) 97.8 (0.2) 91.6 (0.0) 51.8 (0.2) 81.7 (-) 85.7\n4.1 Linear probing vs. \ufb01ne-tuning\nExperiment protocols. We initialize with the pretrained model, and \ufb01ne-tune or linear probe on ID training\nexamples. For \ufb01ne-tuning on each dataset we swept over 6 learning rates, using a cosine learning rate schedule\nand batch size of 64. We early stop and choose the best learning rate using ID validation accuracy. For linear\nprobing we train an \u21132-regularized logistic regression classi\ufb01er on frozen features from the penultimate layer\nof the pretrained model, selecting the best \u21132-regularization hyperparameter based on ID validation accuracy.\nFor all methods, we run each hyperparameter con\ufb01guration 3 times (with different random seeds), and take\nthe average accuracy. We used a slightly different protocol for ImageNet because the dataset is much larger\nand running these experiments involves more computational resources: we used a batch size of 128, swept\nover 3 learning rates for both \ufb01ne-tuning and linear probing (we did not sweep over \u21132-regularization), and\nran each hyperparameter con\ufb01guration once. In all cases, OOD data was only used for evaluation.\nResults. Fine-tuning does better than linear probing on 5 out of 6 ID datasets (average accuracy of 85.1%\nfor \ufb01ne-tuning vs. 82.9% for linear probing, see Table 1). This is consistent with prior work and intuitions.\nHowever, linear-probing does better on 8 out of 10 OOD datasets (average accuracy of 66.2% for linear\nprobing vs. 59.3% for \ufb01ne-tuning, see Table 2)\u2014linear probing does better on all datasets except CIFAR-10.1\nand ImageNetV2, where the OOD is designed to closely replicate the ID dataset. This matches our theoretical\npredictions, which says that linear probing does better than \ufb01ne-tuning when the ID and OOD are very\ndifferent (and the pretrained features are \u201cgood\u201d). Our training datasets vary in size from 20K examples to\nover a million examples, so linear probing does not appear to perform better than \ufb01ne-tuning simply because\nof a small training set.\n4.2 Linear probing then \ufb01ne-tuning (LP-FT)\nExperiment protocols. For LP-FT, we initialize the neural network head using the linear probed solution,\nand then \ufb01ne-tune the model. LP-FT and \ufb01ne-tuning use similar compute because the linear probing step is\nmuch faster than \ufb01ne-tuning. As with \ufb01ne-tuning, we swept over 6 learning rates, early stopping using ID\nvalidation accuracy. For the ImageNet experiments we swept over 3 learning rates, and explicitly ensured that\nLP-FT and \ufb01ne-tuning use exactly the same compute (we ran each stage of LP-FT for half as many epochs as\nwe ran vanilla \ufb01ne-tuning).\nResults. We \ufb01nd that LP-FT gets the best accuracy ID (average: 85.7%) and OOD (average: 68.9%). This is\ntrue for 5/6 ID and 10/10 OOD datasets\u2014every dataset except FMoW ID, where LP-FT is better than linear\nprobing but worse than \ufb01ne-tuning. Since the ID accuracy on FMoW is low (56.5%), this could be because\n12",
    "Table 2: OOD accuracies with 90% con\ufb01dence intervals over 3 runs. Linear probing does better than\n\ufb01ne-tuning on all datasets except CIFAR-10.1 and ImageNetV2, where the ID and OOD are very similar (this\nis consistent with our theory). LP-FT matches or exceeds \ufb01ne-tuning and linear probing on all 10 datasets.\nSTL CIFAR-10.1 Ent-30 Liv-17 DomainNet FMoW\nFT 82.4 (0.4) 92.3 (0.4) 60.7 (0.2) 77.8 (0.7) 55.5 (2.2) 32.0 (3.5)\nLP 85.1 (0.2) 82.7 (0.2) 63.2 (1.3) 82.2 (0.2) 79.7 (0.6) 36.6 (0.0)\nLP-FT 90.7 (0.3) 93.5 (0.1) 62.3 (0.9) 82.6 (0.3) 80.7 (0.9) 36.8 (1.3)\nImNetV2 ImNet-R ImNet-Sk ImNet-A Average\nFT 71.5 (-) 52.4 (-) 40.5 (-) 27.8 (-) 59.3\nLP 69.7 (-) 70.6 (-) 46.4 (-) 45.7 (-) 66.2\nLP-FT 71.6 (-) 72.9 (-) 48.4 (-) 49.1 (-) 68.9\nthe pretrained features are not good.\n4.3 Examining the feature distortion theory\nEarly stopping does not mitigate feature distortion. One might think that \ufb01ne-tuning is simply over\ufb01tting\nID, and so early stopping on OOD data (if it were available) might match linear probing OOD. However, our\ntheory predicts that \ufb01ne-tuning can do worse OOD (than linear probing) throughout the process of \ufb01ne-tuning,\nand not just at the end. To test this, we early stop each \ufb01ne-tuning method and choose the best learning rate\nbased on OOD test accuracy (OOD data was not used except for this ablation). As expected, \ufb01ne-tuning\ndoes improve a little, but linear probing (average accuracy: 67.1%) is still better than \ufb01ne-tuning (average\naccuracy: 61.3%). See Appendix B for per-dataset results.\nID-OOD features get distorted from \ufb01ne-tuning. The feature distortion theory predicts that \ufb01ne-tuning\nchanges features for ID examples more than for OOD examples, which is why \ufb01tting a head on ID examples\nperforms poorly OOD. To test this, for each example xin Living-17 (results for other datasets are in\nAppendix B), we took the Euclidean distance of the ResNet-50 features before and after \ufb01ne-tuning: \u2225gB(x)\u2212\ngB0(x)\u22252. As expected, the average distance for ID examples ( 0.0188\u00b10.0001 ) is more than for OOD\nexamples ( 0.0167\u00b10.0001 ). The theory also predicts that LP-FT changes features less than \ufb01ne-tuning does.\nAs expected, the average distance changed by LP-FT both ID ( 0.0011\u00b10.0001 ) and OOD ( 0.0009\u00b10.0001 )\nis20\u00d7smaller than for \ufb01ne-tuning.\nPretrained features must be good, ID-OOD far apart. Our theory gives conditions under which linear\nprobing can do better than \ufb01ne-tuning OOD. Speci\ufb01cally, we require that the ID distribution Pidand OOD\ndistribution Poodare quite different, and the pretrained features are good ( B0is close toB\u22c6)\u2014otherwise\n\ufb01ne-tuning can do better OOD by adjusting the feature extractor ID. Here we test that these conditions are\nessential\u2014when they are violated \ufb01ne-tuning can do better than linear probing OOD.\nFeature quality : We use a checkpoint of MoCo-v1 that got 10% worse accuracy (on ImageNet) and compare\nlinear probing and \ufb01ne-tuning on Living-17. With worse features, both methods do worse, but \ufb01ne-tuning\n(96% ID, 71% OOD) does better than linear probing (92% ID, 66% OOD).\n13",
    "ID\u2248OOD : We \ufb01ne-tune / linear probe on CIFAR-10, and test on CIFAR-10.1, a dataset collected using a\nsimilar protocol to CIFAR-10. As expected, \ufb01ne-tuning (92.3%) outperforms linear probing OOD (82.7%).\nEven in this case, where we have no tradeoffs, LP-FT does the best (93.5%).\n5 Related work and discussion\nFine-tuning vs. linear probing. Fine-tuning (FT) and linear probing (LP) are popular transfer learning\nalgorithms. There is substantial evidence of FT outperforming LP in-distribution (ID) including recent\nlarge-scale investigations (Kornblith et al., 2019; Chen et al., 2021a; Zhai et al., 2020; Chen et al., 2020b)\n(the only notable exception is in Peters et al. (2019) where LP performs better than FT when using ELMo\nrepresentations, but worse using BERT).9FT is therefore the method of choice for improving accuracy,\nwhile LP is used to analyze properties of representations (Peters et al., 2018; Belinkov et al., 2017; Hewitt\n& Manning, 2019). In our work, we \ufb01nd that FT can underperform LP especially when using high quality\npretrained features in the presence of a large distribution shift. There are a variety of other \ufb01ne-tuning\nheuristics (Ge & Yu, 2017; Guo et al., 2019; Zhang et al., 2020; Zhu et al., 2020; Jiang et al., 2021;\nAghajanyan et al., 2021)\u2014combining our insights with these ideas might lead to better methods.\nThe bene\ufb01t of preserving pretrained features. Our work adds to growing evidence that lightweight\n\ufb01ne-tuning, where only a small part of a pretrained model are updated, performs better under distribution\nshifts\u2014and we give a theoretical grounding to why this might be the case. Zero-shot language prompting in\nvision (Radford et al., 2021) and other lightweight \ufb01ne-tuning approaches in NLP (Houlsby et al., 2019; Li &\nLiang, 2021; Xie et al., 2021b; Lester et al., 2021; Utama et al., 2021; Zhou et al., 2021) have been shown\nto improve OOD performance. In independent and concurrent work, Andreassen et al. (2021) observe that\nthrough the course of \ufb01ne-tuning, ID accuracy continues to increase but OOD accuracy plateaus. Our work\nshows something stronger: at no point in the \ufb01ne-tuning process does FT outperform LP.\nMitigating ID-OOD tradeoffs. While LP-FT has sometimes been used as a \ufb01ne-tuning heuristic (Levine\net al., 2016; Kanavati & Tsuneki, 2021; fastai), it has not been used for robustness / OOD accuracy, and we\nshow that it addresses the ID-OOD tradeoff theoretically and empirically. Tradeoffs between ID and OOD\naccuracy are widely studied and prior work self-trains on large amounts of unlabeled data to mitigate such\ntradeoffs (Raghunathan et al., 2020; Xie et al., 2021a; Khani & Liang, 2021). In contrast, LP-FT uses no\nextra unlabeled data and is a simple variant of \ufb01ne-tuning. In concurrent and independent work, Wortsman\net al. (2021) show that ensembling the weights of a zero-shot and \ufb01ne-tuned model mitigates the ID-OOD\ntradeoff between these approaches, and this method could be promising for our datasets as well.\nTheoretical analysis of transfer learning. Prior works on transfer learning mainly analyze linear prob-\ning (Wu et al., 2020; Tripuraneni et al., 2020; Du et al., 2020). In recent work, (Chua et al., 2021) study\nregularized \ufb01ne-tuning in an underparameterized regime where there is a unique global optimum. In contrast,\nour analysis studies the overparameterized regime (mirroring modern settings of zero train loss) where we\nneed to analyze the trajectory of \ufb01ne-tuning from the pretrained initialization because there is no unique\noptimizer of the objective function. Prior works also focus on ID error, while we analyze OOD error. See\nSection C for additional related work on theory of overparameterized models.\n9This is not intended to be a comprehensive list. There is a large body of past work across different domains that have reported a\nsimilar observation.\n14",
    "6 Conclusion.\nThere is a strong trend towards leveraging pretrained models to improve downstream performance, and\nwhenever feasible, it is common to \ufb01ne-tune all model parameters. In this work, we show theoretically\nand empirically that preserving features might be important for robustness, and simpler approaches like\nlinear-probing can improve out-of-distribution (OOD) performance. This OOD gap between \ufb01ne-tuning and\nlinear probing grows as the quality of pretrained features improve, so we believe our results are likely to gain\nsigni\ufb01cance over time with growing innovations and scale of pretraining .\nTheoretical understanding of modern deep learning remains limited, especially the effect of pretraining and\ntransfer learning. In addition to our speci\ufb01c results on \ufb01ne-tuning, our work introduces some tools and ideas\nfor dealing with the main challenge of characterizing properties of the trajectory from a speci\ufb01c initialization\nin the presence of multiple global optima (implicit regularization effect of initialization). There are several\nopen questions and extensions such as dealing with non-linear activations, different layerwise learning rates,\nand the effect of explicit regularization.10\nFinally, we showed LP-FT can mitigate tradeoffs between ID and OOD accuracy in our context. LP-FT\ncould be useful in other situations, for example in CLIP we could initialize the \ufb01nal layer with the zero-shot\nclassi\ufb01er and then \ufb01ne-tune the entire model, as done in concurrent work (Wortsman et al., 2021). LP-FT is\njust a \ufb01rst step in leveraging the intuition from our theoretical analysis and we hope that this work inspires\nnew methods of leveraging powerful pretrained models.\nProofs and Reproducibility : We include proofs for our theoretical results in Appendix A and additional\nexperiment details in Appendix B.\nAcknowledgements : We would like to thank Kumar Ayush and Burak Uzkent for MoCo checkpoints\npretrained on unlabeled FMoW images, Nilesh Tripuraneni for clari\ufb01cations on his work and references\non principal angles, Daniel Levy for useful suggestions on experiments to run, Niladri Chatterji, Jeff Z.\nHaoChen, and Colin Wei for useful papers and comments on \ufb01gures, Niladri Chatterji and Kaidi Cao for\nreviewing the paper at ML paper swap, Kevin Yang for his help with analyzing differential equations, Tri Dao\nand Pang Wei Koh for help with writing, Suriya Gunasekar, Adam Kalai, Simon Kornblith, Ting Chen, Sang\nMichael Xie, Albert Gu, and Kendrick Shen for useful discussions, and Pang Wei Koh, Niladri Chatterji, and\nTri Dao for suggestions on framing our results better.\nAnanya Kumar was supported by the Rambus Corporation Stanford Graduate Fellowship. Percy Liang\nwas supported by the Open Philantropy Project and NSF Award Grant No. 1805310. Aditi Raghunathan\nwas supported by a Google PhD Fellowship and Open Philanthropy Project AI Fellowship. Tengyu Ma\nacknowledges support of a Google Faculty Award, NSF IIS 2045685, the Sloan Fellowship, JD.com, SAIL,\nand SDSI.\n10We found that LP-FT outperforms explicit regularization and using a higher learning rate for the linear layer on Living-17\n(Appendix B.4), but a more extensive theoretical and empirical study on this is important.\n15",
    "References\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal Gupta.\nBetter \ufb01ne-tuning by reducing representational collapse. In International Conference on Learning Repre-\nsentations (ICLR) , 2021.\nEA AlBadawy, A Saha, and MA Mazurowski. Deep learning for segmentation of brain tumors: Impact of\ncross-institutional training and testing. Med Phys. , 45, 2018.\nAnders Andreassen, Yasaman Bahri, Behnam Neyshabur, and Rebecca Roelofs. The evolution of out-of-\ndistribution robustness throughout \ufb01ne-tuning. arXiv , 2021.\nSanjeev Arora, Nadav Cohen, and Elad Hazan. On the optimization of deep networks: Implicit acceleration\nby overparameterization. In International Conference on Machine Learning (ICML) , pp. 244\u2013253, 2018.\nKumar Ayush, Burak Uzkent, Chenlin Meng, Kumar Tanmay, M. Burke, D. Lobell, and Stefano Ermon.\nGeography-aware self-supervised learning. arXiv , 2020.\nPeter L. Bartlett, Philip M. Long, G /acute.ts1abor Lugosi, and Alexander Tsigler. Benign over\ufb01tting in linear\nregression. arXiv , 2019.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Hassan Sajjad, and James Glass. What do neural machine\ntranslation models learn about morphology? In Association for Computational Linguistics (ACL) , pp.\n861\u2013872, 2017.\nMikhail Belkin, Daniel Hsu, and Ji Xu. Two models of double descent for weak features. arXiv , 2019.\nKoby Bibas, Yaniv Fogel, and Meir Feder. A new look at an old problem: A universal learning approach to\nlinear regression. In 2019 IEEE International Symposium on Information Theory (ISIT) , pp. 2304\u20132308,\n2019.\nTianle Cai, Ruiqi Gao, J. Lee, and Qi Lei. A theory of label propagation for subpopulation shift. In\nInternational Conference on Machine Learning (ICML) , 2021.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive\nlearning of visual representations. In International Conference on Machine Learning (ICML) , pp. 1597\u2013\n1607, 2020a.\nXinlei Chen, Haoqi Fan, Ross B. Girshick, and Kaiming He. Improved baselines with momentum contrastive\nlearning. arXiv , 2020b.\nXinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers.\narXiv preprint arXiv:2104.02057 , 2021a.\nYining Chen, Elan Rosenfeld, Mark Sellke, Tengyu Ma, and Andrej Risteski. Iterative feature matching:\nToward provable domain generalization with logarithmic environments. arXiv , 2021b.\nGordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In\nComputer Vision and Pattern Recognition (CVPR) , 2018.\nKurtland Chua, Qi Lei, and Jason D Lee. How \ufb01ne-tuning allows for effective meta-learning. arXiv preprint\narXiv:2105.02221 , 2021.\n16",
    "Adam Coates, Andrew Ng, and Honlak Lee. An analysis of single-layer networks in unsupervised feature\nlearning. In Proceedings of the Fourteenth International Conference on Arti\ufb01cial Intelligence and Statistics ,\nvolume 15, pp. 215\u2013223, 2011.\nSimon S. Du, Wei Hu, Sham M. Kakade, Jason D. Lee, and Qi Lei. Few-shot learning via learning the\nrepresentation, provably. arXiv , 2020.\nSimon Shaolei Du, Wei Hu, and Jason Lee. Algorithmic regularization in learning deep homogeneous models:\nLayers are automatically balanced. In Advances in Neural Information Processing Systems (NeurIPS) ,\n2018.\nfastai. fastai tutorial on transfer learning. https://github.com/fastai/course-v3/blob/\nmaster/nbs/dl1/lesson1-pets.ipynb .\nGeoff French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual domain adaptation. In\nInternational Conference on Learning Representations , 2018.\nWeifeng Ge and Yizhou Yu. Borrowing treasures from the wealthy: Deep transfer learning through selective\njoint \ufb01ne-tuning. In Computer Vision and Pattern Recognition (CVPR) , 2017.\nGauthier Gidel, Francis R. Bach, and Simon Lacoste-Julien. Implicit regularization of discrete gradient\ndynamics in deep linear neural networks. In Advances in Neural Information Processing Systems (NeurIPS) ,\n2019.\nXavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural networks.\nInInternational Conference on Arti\ufb01cial Intelligence and Statistics , 2010.\nGene H. Golub and Charles F. Van Loan. Matrix Computations . The Johns Hopkins University Press, 2013.\nSuriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati Srebro. Implicit\nregularization in matrix factorization. In Advances in Neural Information Processing Systems (NeurIPS) ,\npp. 6151\u20136159, 2017.\nYunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and Rogerio Feris. Spottune:\nTransfer learning through adaptive \ufb01ne-tuning. In Computer Vision and Pattern Recognition (CVPR) , 2019.\nTrevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan J Tibshirani. Surprises in high-dimensional\nridgeless least squares interpolation. arXiv preprint arXiv:1903.08560 , 2019.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In Computer Vision and Pattern Recognition (CVPR) , 2020.\nDan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and\nuncertainty. In International Conference on Machine Learning (ICML) , 2019a.\nDan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples.\narXiv preprint arXiv:1907.07174 , 2019b.\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai,\nTyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces\nof robustness: A critical analysis of out-of-distribution generalization. arXiv preprint arXiv:2006.16241 ,\n2020.\n17",
    "John Hewitt and Christopher D. Manning. A structural probe for \ufb01nding syntax in word representations. In\nAssociation for Computational Linguistics (ACL) , 2019.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-ef\ufb01cient transfer learning for NLP. arXiv , 2019.\nJeremy Howard and Sebastian Ruder. Universal language model \ufb01ne-tuning for text classi\ufb01cation. In\nAssociation for Computational Linguistics (ACL) , 2018.\nNeal Jean, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon. Combining\nsatellite imagery and machine learning to predict poverty. Science , 353, 2016.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart: Robust and\nef\ufb01cient \ufb01ne-tuning for pre-trained natural language models through principled regularized optimization.\nInInternational Conference on Learning Representations (ICLR) , 2021.\nPritish Kamath, Akilesh Tangella, Danica J. Sutherland, and Nathan Srebro. Does invariant risk minimization\ncapture invariance? In Arti\ufb01cial Intelligence and Statistics (AISTATS) , 2021.\nFahdi Kanavati and Masayuki Tsuneki. Partial transfusion: on the expressive in\ufb02uence of trainable batch\nnorm parameters for transfer learning. In Medical Imaging with Deep Learning , 2021.\nFereshte Khani and Percy Liang. Removing spurious features can hurt accuracy and affect groups dispropor-\ntionately. In ACM Conference on Fairness, Accountability, and Transparency (FAccT) , 2021.\nPang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani,\nWeihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, Tony Lee, Etienne David, Ian Stavness,\nWei Guo, Berton A. Earnshaw, Imran S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma\nPierson, Sergey Levine, Chelsea Finn, and Percy Liang. WILDS: A benchmark of in-the-wild distribution\nshifts. In International Conference on Machine Learning (ICML) , 2021.\nSimon Kornblith, Jonathon Shlens, and Quoc V . Le. Do better imagenet models transfer better? In Computer\nVision and Pattern Recognition (CVPR) , 2019.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, University of\nToronto, 2009.\nThomas Laurent and James H. von Brecht. Deep linear neural networks with arbitrary loss: All local minima\nare global. In International Conference on Machine Learning (ICML) , 2018.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-ef\ufb01cient prompt tuning.\narXiv preprint arXiv:2104.08691 , 2021.\nS. Levine, Chelsea Finn, Trevor Darrell, and P. Abbeel. End-to-end training of deep visuomotor policies.\nJournal of Machine Learning Research (JMLR) , 17, 2016.\nXiang Lisa Li and Percy Liang. Pre\ufb01x-tuning: Optimizing continuous prompts for generation. In Association\nfor Computational Linguistics (ACL) , 2021.\nXuhong Li, Yves Grandvalet, and Franck Davoine. Explicit inductive bias for transfer learning with\nconvolutional networks. In International Conference on Machine Learning (ICML) , 2018.\n18",
    "Song Mei and Andrea Montanari. The generalization error of random features regression: Precise asymptotics\nand double descent curve. arXiv preprint arXiv:1908.05355 , 2019.\nJohn Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy\nLiang, Yair Carmon, and Ludwig Schmidt. Accuracy on the line: on the strong correlation between\nout-of-distribution and in-distribution generalization. In International Conference on Machine Learning\n(ICML) , 2021.\nVidya Muthukumar, Kailas V odrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of\nnoisy data in regression. IEEE Journal on Selected Areas in Information Theory , 1(1):67\u201383, 2020.\nBehnam Neyshabur, Ryota Tomioka, and Nathan Srebro. In search of the real inductive bias: On the role of\nimplicit regularization in deep learning. arXiv , 2014.\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for\nmulti-source domain adaptation. In International Conference on Computer Vision (ICCV) , 2019.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In North American Association for Computational\nLinguistics (NAACL) , 2018.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of the 4th Workshop on Representation Learning for NLP\n(RepL4NLP-2019) , pp. 7\u201314, 2019.\nViraj Prabhu, Shivam Khare, Deeksha Karthik, and Judy Hoffman. Selective entropy optimization via\ncommittee consistency for unsupervised domain adaptation. In International Conference on Computer\nVision (ICCV) , 2021.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning\ntransferable visual models from natural language supervision. In International Conference on Machine\nLearning (ICML) , volume 139, pp. 8748\u20138763, 2021.\nAditi Raghunathan, Sang Michael Xie, Fanny Yang, John C. Duchi, and Percy Liang. Understanding and\nmitigating the tradeoff between robustness and accuracy. In International Conference on Machine Learning\n(ICML) , 2020.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do CIFAR-10 classi\ufb01ers\ngeneralize to CIFAR-10? arXiv , 2018.\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classi\ufb01ers generalize\nto imagenet? In International Conference on Machine Learning (ICML) , 2019.\nElan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski. The risks of invariant risk minimization. In\nInternational Conference on Learning Representations (ICLR) , 2021.\nMark Rudelson and Roman Vershynin. Smallest singular value of a random rectangular matrix. Communica-\ntions on Pure and Applied Mathematics , 62:1707\u20131739, 2009.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\n19",
    "Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge.\nInternational Journal of Computer Vision , 115(3):211\u2013252, 2015.\nShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift.\narXiv , 2020.\nAndrew M. Saxe, James L. McClelland, and Surya Ganguli. Exact solutions to the nonlinear dynamics of\nlearning in deep linear neural networks. arXiv , 2014.\nShuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: An empirical odyssey.\narXiv preprint arXiv:1910.10320 , 2020.\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt.\nMeasuring robustness to natural distribution shifts in image classi\ufb01cation. arXiv preprint arXiv:2007.00644 ,\n2020.\nNilesh Tripuraneni, Michael I. Jordan, and Chi Jin. On the theory of transfer learning: The importance of\ntask diversity. arXiv , 2020.\nJoel A. Tropp. An introduction to matrix concentration inequalities. Foundations and Trends in Machine\nLearning , 8:1\u2013230, 2015.\nPrasetya Ajie Utama, Na\ufb01se Sadat Moosavi, Victor Sanh, and Iryna Gurevych. Avoiding inference heuristics\nin few-shot prompt-based \ufb01netuning. arXiv preprint arXiv:2109.04144 , 2021.\nHaohan Wang, Songwei Ge, Zachary Lipton, and Eric P Xing. Learning robust global representations by\npenalizing local predictive power. In Advances in Neural Information Processing Systems (NeurIPS) , 2019.\nMitchell Wortsman, Gabriel Ilharco, Mike Li, Jong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. Robust \ufb01ne-tuning of zero-shot models. arXiv preprint\narXiv:2109.01903 , 2021.\nSen Wu, Hongyang R. Zhang, and Christopher R\u00e9. Understanding and improving information transfer in\nmulti-task learning. In International Conference on Learning Representations (ICLR) , 2020.\nSang Michael Xie, Ananya Kumar, Robbie Jones, Fereshte Khani, Tengyu Ma, and Percy Liang. In-N-out:\nPre-training and self-training using auxiliary information for out-of-distribution robustness. In International\nConference on Learning Representations (ICLR) , 2021a.\nSang Michael Xie, Tengyu Ma, and Percy Liang. Composed \ufb01ne-tuning: Freezing pre-trained denoising\nautoencoders for improved generalization. In International Conference on Machine Learning (ICML) ,\n2021b.\nFisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and\nTrevor Darrell. Bdd100k: A diverse driving dataset for heterogeneous multitask learning. In Computer\nVision and Pattern Recognition (CVPR) , 2020.\nXiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip\nDjolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem,\nMichael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale\nstudy of representation learning with the visual task adaptation benchmark. arXiv , 2020.\n20",
    "Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas Guibas, and Jitendra Malik. Side-tuning: A baseline\nfor network adaptation via additive side networks. In European Conference on Computer Vision (ECCV) ,\n2020.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language\nmodels. arXiv preprint arXiv:2109.01134 , 2021.\nChen Zhu, Yu Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu. FreeLB: Enhanced adversarial\ntraining for natural language understanding. In International Conference on Learning Representations\n(ICLR) , 2020.\n21",
    "A Proofs for Section 3\nA.1 Preliminaries on Important Notations and Principal Angles\nBig-Oh Notation : For convenience, we use big-oh notation in a way that differs from standard theoretical\ncomputer science texts. When we say O(<expr1> )we mean that this can be replaced by c<expr1> for some\nuniversal constant such that the statement holds. As an example, we can say 5x2\u2264O(x2)because there\nexists some universal constant ( c= 5) such that 5x2\u22645x2. More examples: we can also say 5x2\u2265O(x2)\nor ifx\u22651then7x2\u2264O(x3)and0.1x2\u2265O(x).\nSingular Values : Given a rectangular matrix A\u2208Rm\u00d7n, letr= min(m,n). The minimum singular value\nis de\ufb01ned as the r-th largest singular value of A, so\u03c3min(A) =\u03c3r(A).\nWorking with minimum singular values requires more care than maximum singular vectors. In particular,\nwhen we have rectangular matrices some bounds depend on whether the matrix is \u2018fat\u2019 (has more columns\nthan rows) or \u2018tall\u2019 (has more rows than columns).\nGiven a matrix A, the operator norm \u2225A\u22252is the maximum singular value: \u2225A\u22252=\u03c3max(A).\nProjectors : Given a subspace RofRd, let\u03a0Rdenote the orthogonal projection onto R, satisfying that for\nallx\u2208Rd:\n\u03a0R(x)\u2208Rand\u2200r\u2208R,\u2225x\u2212\u03a0R(x)\u22252\u2264\u2225x\u2212r\u22252. (A.1)\nIfE\u2208Rd\u00d7dim(R)has orthonormal columns that form a basis for R, then we have:\n\u03a0R=EE\u22a4(A.2)\nFrom this we can easily check that \u03a02\nR= \u03a0Rand\u03a0\u22a4\nR= \u03a0R. See e.g., Chapter 2.5.1 Golub & Loan (2013)\nfor more information.\nPrincipal Angles : Given two non-zero vectors xandy, the cosine of the angle between them, cos\u03b8, is:\ncos\u03b8=x\u22a4y\n\u2225x\u22252\u2225y\u22252(A.3)\nIf we consider the 1-dimensional subspaces (so basically lines) SxandSyspanned byxandyrespectively,\nthen the angle between them, cos\u03b8\u2032is given by the absolute value (since lines are undirected):\ncos\u03b8\u2032=|x\u22a4y|\n\u2225x\u22252\u2225y\u22252(A.4)\nPrincipal angles generalize this notion to higher dimensions. See e.g., Chapter 6.4.3 in Golub & Loan (2013)\nfor more information on principal angles.\nDe\ufb01nition A.1. Given two non-empty subspaces RandSofRd, wherer= min(dim( R),dim(S)), we have\nrprincipal angles:\n0\u2264\u03b81\u2264...\u2264\u03b8r\u2264\u03c0/2. (A.5)\nThe directions of the inequalities swap when we take the cosine of the principal angles:\n1\u2265cos\u03b81\u2265...\u2265cos\u03b8r\u22640. (A.6)\n22",
    "The cosines of the principal angles are given by the SVD\u2014let E\u2208Rd\u00d7dim(R)andF\u2208Rd\u00d7dim(S)have\northonormal columns which span RandSrespectively. Then we have:\ncos\u03b8i=\u03c3i(E\u22a4F), (A.7)\nwhere\u03c3idenotes thei-th largest singular value. In this paper, we are interested in the cosine of the largest\nangle between them, given by:\ncos\u03b8max(R,S) = cos\u03b8r (A.8)\nWe can massage this into a variational characterization of the maximum principal angle, which is important\nfor lower bounding the error of \ufb01ne-tuning outside the span of the training data.\nLemma A.2. Suppose dim(R)\u2264dim(S), and letF\u2208Rd\u00d7dim(S)have orthonormal columns that form a\nbasis forS. We have:\ncos\u03b8max(R,S) = min\nr\u2208R,\u2225r\u22252=1\u2225F\u22a4(r)\u22252 (A.9)\nProof. LetE\u2208Rd\u00d7dim(R)andF\u2208Rd\u00d7dim(S)have orthonormal columns that span RandSrespectively.\nSince dim(R)\u2264dim(S)(a crucial condition!), F\u22a4Eis a \u2018tall\u2019 matrix (it has more rows than columns) so\nwe have:\n\u03c3min(F\u22a4E) = min\n\u2225v\u22252=1\u2225F\u22a4Ev\u22252. (A.10)\nThe result now follows from some algebra:\ncos\u03b8max(R,S) =\u03c3min(F\u22a4E) (A.11)\n= min\n\u2225v\u22252=1\u2225F\u22a4Ev\u22252 (A.12)\n= min\nr\u2208R,\u2225r\u22252=1\u2225F\u22a4(r)\u22252. (A.13)\nA.2 Feature distortion theorem\nWe \ufb01rst prove our core theorem, that \ufb01ne-tuning distorts pretrained features.\nRestatement of Theorem 3.3. In the overparameterized linear setting, let S\u22a5=rowspace (X)\u22a5,R0=\nrowspace (B0), andv\u22c6,B\u22c6be the optimal parameters with w\u22c6=B\u22c6v\u22c6. Ifcos\u03b8max(R0,S\u22a5)>0, then for\nall time steps t, the OOD error of the \ufb01ne-tuning iterates (Bft(t),vft(t))is lower bounded:\n\u221a\nLood(vft(t),Bft(t))\u2265\u221a\n\u03c3min(\u03a3)(cos\u03b8max(R0,S\u22a5)\u221a\nkmin(\u03d5,\u03d52/\u2225w\u22c6\u22252)\n(1 +\u2225w\u22c6\u22252)2\u2212\u03f5)\n, (A.14)\nwhere\u03d52=|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|is de\ufb01ned to be inital head alignment error and \u03f5\u2265d(B0,B\u22c6)is the error\nin the pretrained feature extractor.\nWe follow the sketch in the main paper. We begin with a few lemmas, showing that certain quantities are\npreserved throughout the \ufb01ne-tuning process.\n23",
    "Our \ufb01rst lemma says that the representations Bt\nftxdo not change for examples perpendicular the span of the\ntraining examples. Note that the \ufb01nal output vt\nft\u22a4Bt\nftxstill changes, because vt\nftchanges.\nLemma A.3. For all times tand allx\u2208S\u22a5, we have:\nB0x=Bt\nftx (A.15)\nProof. We initialized \ufb01ne-tuning with the feature extractor Bft(0) =B0. It suf\ufb01ces to show that \u2202tBt\nftx= 0\nfor allx\u2208S\u22a5. Recall that \u2202tBt\nftis given by the gradient \ufb02ow update equation:\n\u2202tBt\nft=\u2212\u2202B\u02c6L(vt\nft,Bt\nft) =\u2212\u2202B\u2225XB\u22a4v\u2212Y\u22252\n2 (A.16)\nComputing the RHS explicitly using multivariable chain rule, we get:\n\u2202tBt\nft=\u22122v(XB\u22a4v\u2212Y)\u22a4X (A.17)\nSincexis a constant, we get:\n\u2202tBt\nftx=\u22122v(XB\u22a4v\u2212Y)\u22a4Xx (A.18)\nButXx= 0 forx\u2208S\u22a5, sincex\u2208S\u22a5is de\ufb01ned as xis perpendicular to the rowspace of X(i.e.,\nperpendicular to the rows of X). So the RHS is 0\u2014that is,\u2202tBt\nftx= 0, as desired.\nNext, we show that the change in the head and feature extractor are \u2018coupled\u2019. So if the head changes in a\ncertain way, then the feature extractor cannot just stay the same. In the literature, this is sometimes called the\n\u201cbalancedness\" lemma, and has been proved in prior work on two layer linear networks.\nLemma A.4. For alltwe have:\nv0v\u22a4\n0\u2212B0B\u22a4\n0=vt\nftvt\nft\u22a4\u2212Bt\nftBt\nft\u22a4(A.19)\nProof. This follows by showing that the derivative is 0:\n\u2202t[vt\nftvt\nft\u22a4\u2212Bt\nftBt\nft\u22a4] = 0 (A.20)\nWhich can be veri\ufb01ed by direct calculation. See Theorem 2.2 in Du et al. (2018) and the proof of Theorem 1\nin Arora et al. (2018).\nFor our proof we will require that every feature r\u2208Rcan be generated from some OOD direction, that\nisr=B0ufor someu\u2208S\u22a5. We will show that this is implied by the condition on the principal angle:\ncos\u03b8max(R,S\u22a5)>0whereR=rowspace (B0), which we assumed in Theorem 3.3. The following lemma\nshows this (and also quanti\ufb01es that the norm of udoes not shrink too much when projected onto R).\nLemma A.5. LetR,S be subspaces of Rdwithdim(R)\u2264dim(S). For allr\u2208Rwith\u2225r\u22252= cos\u03b8max(R,S),\nthere existss\u2208Swith\u03a0R(s) =rand\u2225s\u22252\u22641. Here \u03a0R\u2208Rd\u00d7dprojects a vector onto R.\nProof. Letc= cos\u03b8max(R,S). Firt, we get rid of an easy case\u2014if c= 0, then we need to show the claim\nfor allr\u2208Rwith\u2225r\u22252=c= 0, which means r= 0. Then we can just pick s= 0, and \u03a0R(s) = 0 =rand\n\u2225s\u22252= 0\u22641. So for the rest of the proof we assume c>0.\n24",
    "Consider arbitrary vector r\u2208Rwith\u2225r\u22252=c. LetE\u2208Rd\u00d7dim(S),F\u2208Rd\u00d7dim(R)have orthonormal\ncolumns, which form a basis for RandSrespectively.\nStep 1: Finding s: Since the columns of EspanR,r=Ezfor somez\u2208Rdim(R).c=\u03c3min(E\u22a4F)>0,\nwhich means that E\u22a4F\u2208Rdim(R)\u00d7dim(S)has rank dim(R)since dim(R)\u2264dim(S)\u2014in other words,\nE\u22a4Fhas full column rank since the column dimension is smaller than the row dimension. So z=E\u22a4Fw\nfor somew\u2208rowspace (E\u22a4F). Then we set s=Fw\u2014this means s\u2208Sbecause the columns of Fform\na basis forS. In addition, following the steps above we have r=Ez=EE\u22a4Fw=EE\u22a4s. We note that\n\u03a0R=EE\u22a4is the projection onto R(see e.g., Chapter 2.5.1 of Golub & Loan (2013)).\nStep 2: Bounding norm of s: It suf\ufb01ces to show that \u2225s\u22252\u22641. SinceFhas orthonormal columns,\n\u2225s\u22252=\u2225Fw\u22252=\u2225w\u22252, so it suf\ufb01ces to show that \u2225w\u22252\u22641. SinceEhas orthonormal columns, \u2225r\u22252=\u2225z\u22252.\nRecall thatz=E\u22a4Fw\u2014sincew\u2208rowspace (E\u22a4F), from Lemma A.6 we have:\n\u2225z\u22252\u2265\u03c3min(E\u22a4F)\u2225w\u22252=c\u2225w\u22252. (A.21)\nRearranging, we get \u2225w\u22252\u2264\u2225z\u22252/c= 1, as desired.\nIn the lemma above, we used a standard linear algebraic result that we include for completeness. This says\nthatAcannot shrink vectors in its rowspace too much, where the shrinkage factor is given by the minimum\nsingular value of A.\nLemma A.6. LetA\u2208Rm\u00d7n. Letr= min(m,n). Then ifx\u2208rowspace (A), we have\u2225Ax\u22252\u2265\u03c3r(A)\u2225x\u22252.\nProof. We bound the norm of xusing the SVD. Consider the singular value decomposition (SVD) of A:\nA=UDV\u22a4(A.22)\nWhereU\u2208Rm\u00d7r,D\u2208Rr\u00d7r,V\u22a4\u2208Rr\u00d7n, whereUandVhave orthonormal columns, and D=\ndiag(\u03c31,...,\u03c3r)is a diagonal matrix with \u03c31\u2265...\u2265\u03c3r\u22650.\n\u2225Ax\u22252=\u2225UDV\u22a4x\u22252 [De\ufb01nition of r] (A.23)\n=\u2225DV\u22a4x\u22252 [U\u2208Rm\u00d7rhas orthonormal columns] (A.24)\n\u2265\u03c3r\u2225V\u22a4x\u22252 [Dis diagonal] (A.25)\n=\u03c3r\u2225x\u22252 [rows ofV\u22a4are orthonormal, xis in rowspace] (A.26)\n=\u03c3r(A)\u2225x\u22252 (A.27)\nWhere for the fourth step, we used the fact that if x\u2208rowspace (V\u22a4)and the rows of V\u22a4are orthonormal,\nthen\u2225V\u22a4x\u22252=\u2225x\u22252. One way to see this is by writing x=\u2211\ni\u03b1ivi, whereviare rows ofV\u22a4, and then\nnoting thatV\u22a4x= (\u03b11,...,\u03b1r)and soxandV\u22a4xhave the same norm.\nWe recall that Poodhas second moment \u03a3:E[xx\u22a4] = \u03a3 whenx\u223cPood, where \u03a3is invertible. So with\nsome simple algebra we can write the OOD error Loodin terms of \u03a3(the proof is standard and basic, but we\ninclude it just for completeness):\n25",
    "Lemma A.7.\nLood(v,B) = (B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)\u22a4\u03a3(B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)\u2264\u03c3min(\u03a3)\u2225B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v\u22252\n2. (A.28)\nProof. Letx\u223cPood. We have,\nLood(v,B) =E[(v\u22a4\n\u22c6B\u22c6x\u2212v\u22a4Bx)2] (A.29)\n=E[(B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)\u22a4xx\u22a4(B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)] (A.30)\n= (B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)\u22a4E[xx\u22a4](B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v) (A.31)\n= (B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v)\u22a4\u03a3(B\u22a4\n\u22c6v\u22c6\u2212B\u22a4v). (A.32)\nThe inequality follows immediately because \u03c3min(A)(for a square matrix A) is simply the min over xwith\nunit\u21132norm ofx\u22a4Ax.\nWe now prove Theorem 3.3, following the 3 steps outlined in the main text.\nProof of Theorem 3.3. Letc= cos\u03b8max(R,S\u22a5). From Lemma A.7, we have Lood(vt\nft,Bt\nft)\u2264\u03c3min(\u03a3)\u2225B\u22a4\n\u22c6v\u22c6\u2212\nBt\nft\u22a4vt\nft\u22252\n2so it suf\ufb01ces to bound \u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252.\nBecause it makes the proof much easier, we will prove the contrapositive, and then convert back to the\noriginal theorem statement. We assume \u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252\u2264\u2206, and will show that:\n|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|\u2264\u2206 +\u03f5\ncg1(\u2225w\u22252)\u221a\nk+(\u2206 +\u03f5)2\nc2g2(\u2225w\u22252)k (A.33)\nWhereg1andg2are non-negative polynomials we will bound in the proof.\nWe gave a basic outline of the proof in the main paper, and here we are just trying to be careful about capturing\nall the dependencies. We also give intuition for each step before diving into algebra (which we include for\ncompleteness).\nRecall that in the overparameterized linear setting we assumed we have orthonormal B0with\u2225B0\u2212UB\u22c6\u22252\u2264\u03f5\nfor someU. We note that the setup is rotationally symmetric so without loss of generality we can suppose\n\u2225B0\u2212B\u22c6\u22252\u2264\u03f5. This is because we can let B\u2032\n\u22c6=UB\u22c6andv\u2032\n\u22c6=Uv\u22c6, and we have w\u22c6=B\u22a4\n\u22c6v\u22c6=\n(UB\u22c6)\u22a4(Uv\u22c6), wherew\u22c6is the optimal classi\ufb01er\u2014so we can now write the entire proof in terms of B\u2032\n\u22c6and\nv\u2032\n\u22c6.\nStep 1: Show that \u2225vt\nft\u2212v\u22c6\u22252\u2264\u2206/c: We \ufb01rst give intuition and then dive into the math. The key insight is\nto use the fact that in \u2018many\u2019 directions Bt\nftandB0are the same (formally, for all x\u2208S\u22a5,Bt\nftx=B0x).\nButB0andB\u22c6are close by assumption, which means that Bt\nftandB\u22c6are close in \u2018many\u2019 directions. Then\nsince we assumed in the contrapositive that vt\nft\u22a4Bt\nftandv\u22a4\n\u22c6B\u22c6are close, we get that vt\nftandv\u22c6are close in\n\u2018many\u2019 directions. Because S\u22a5covers the rowspace of B0, we get that \u2018many\u2019 is k, which is precisely the\ndimensionality of v\u22c6, so the two vectors vt\nftandv\u22c6must be close.\nWe now dive into the math. Since B0has orthogonal rows, B0has full column rank.\n26",
    "Letzbe given by:\nz=c\n\u2225vt\nft\u2212v\u22c6\u22252(vt\nft\u2212v\u22c6) (A.34)\nWe note that\u2225z\u22252=c. Then, we can \ufb01nd y\u2208R=rowspace (B0)such thatB0y=z(sinceB0has full\ncolumn-rank) and then \u2225y\u22252=\u2225z\u22252=c(sinceB0has orthonormal rows).\nSincec= cos\u03b8max(R,S\u22a5)>0, andy\u2208Rwith\u2225y\u2225=c, from Lemma A.5 we can choose x\u2208S\u22a5with\n\u2225x\u22252\u22641and\u03a0R(x) =y. Then, we have B0x=z.\nFrom Proposition A.3, since x\u2208S\u22a5,B0does not change in directions of xwhen \ufb01ne-tuning so we have:\nB0x=Bt\nftx.\nThe claim now follows from simple algebraic manipulation, following the intuition we described. The algebra\njust captures what \u2018close\u2019 means and adds up the error terms.\n\u2225vt\nft\u2212v\u22c6\u22252=1\nc(vt\nft\u2212v\u22c6)\u22a4(c(vt\nft\u2212v\u22c6)\n\u2225vt\nft\u2212v\u22c6\u22252) [Algebra] (A.35)\n=1\nc(vt\nft\u2212v\u22c6)\u22a4z [De\ufb01nition of z] (A.36)\n=1\nc(vt\nft\u2212v\u22c6)\u22a4B0x [SinceB0x=z] (A.37)\n=1\nc(vt\nft\u22a4B0x\u2212v\u22a4\n\u22c6B0x) [Algebra] (A.38)\n=1\nc(vt\nft\u22a4Bt\nftx\u2212v\u22a4\n\u22c6B0x) [Bt\nftx=B0xsincex\u2208S\u22a5] (A.39)\n=1\nc(vt\nft\u22a4Bt\nft\u2212v\u22a4\n\u22c6B0)x [Algebra] (A.40)\n\u22641\nc\u2225vt\nft\u22a4Bt\nft\u2212v\u22a4\n\u22c6B0\u22252\u2225x\u22252 [Cauchy-Schwarz] (A.41)\n\u22641\nc\u2225vt\nft\u22a4Bt\nft\u2212v\u22a4\n\u22c6B0\u22252 [since\u2225x\u22252\u22641] (A.42)\n\u22641\nc\u2225vt\nft\u22a4Bt\nft\u2212v\u22a4\n\u22c6B\u22c6\u22252+1\nc\u2225v\u22a4\n\u22c6B\u22c6\u2212v\u22a4\n\u22c6B0\u22252 [Triangle inequality] (A.43)\n\u22641\nc\u2225Bt\nft\u22a4vt\nft\u2212B\u22a4\n\u22c6v\u22c6\u22252+1\nc\u2225v\u22a4\n\u22c6B\u22c6\u2212v\u22a4\n\u22c6B0\u22252 [Taking transpose] (A.44)\n=1\nc\u2225Bt\nft\u22a4vt\nft\u2212B\u22a4\n\u22c6v\u22c6\u22252+1\nc\u03c3max(B0\u2212B\u22c6)\u2225v\u22c6\u22252[de\ufb01nition of max singular value]\n(A.45)\n=1\nc\u2225Bt\nft\u22a4vt\nft\u2212B\u22a4\n\u22c6v\u22c6\u22252+1\nc\u03f5\u2225v\u22c6\u22252 [since\u03c3max(B0\u2212B\u22c6)\u2264\u03f5] (A.46)\n\u2264\u2206 +\u03f5\u2225v\u22c6\u22252\nc[since\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252\u2264\u2206]\n(A.47)\n(A.48)\nWhich shows that\u2225vt\nft\u2212v\u22c6\u22252\u2264(\u2206 +\u03f5\u2225v\u22c6\u22252)/c.\nStep 2A: Show that \u2225Bt\nft\u22252\nFis small : The key insight is to take the trace on both sides of Proposition A.4,\n27",
    "which bounds the Frobenius norm of Bt\nftand therefore the operator norm.\nRearranging Proposition A.4, we have:\nBt\nftBt\nft\u22a4=B0B\u22a4\n0+v\u22c6v\u22a4\n\u22c6\u2212v0v\u22a4\n0 (A.49)\nTaking the trace everywhere, we get:\nTr(Bt\nftBt\nft\u22a4) = Tr(B0B\u22a4\n0) + Tr(v\u22c6v\u22a4\n\u22c6)\u2212Tr(v0v\u22a4\n0) (A.50)\nFor any matrix A,Tr(AA\u22a4) =\u2225A\u22252\nF, and for a vector vthe Frobenius norm is just the \u21132-norm, so\nTr(vv\u22a4) =\u2225v\u22252\n2. So we have:\n\u2225Bt\nft\u22252\nF=\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2\u2212\u2225v0\u22252\n2 (A.51)\nSquares are non-negative, so we get the inequality:\n\u2225Bt\nft\u22252\nF\u2264\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2 (A.52)\nStep 2B: Show that \u2225B\u22a4\n0v\u22c6\u22252\n2\u2212\u2225Bt\nft\u22a4v\u22c6\u22252\n2is small : This step doesn\u2019t involve much insight, and is standard\npeturbation analysis\u2014we simply factor the difference of squares and bound each term.\nFirst, we bound\u2225Bt\nft\u22a4vt\nft\u2212Bt\nft\u22a4v\u22c6\u22252:\n\u2225Bt\nft\u22a4vt\nft\u2212Bt\nft\u22a4v\u22c6\u22252\u2264\u03c3max(Bt\nft)\u2225vt\nft\u2212v\u22c6\u22252 (A.53)\n\u2264\u2225Bt\nft\u2225F\u2225vt\nft\u2212v\u22c6\u22252 (A.54)\n\u2264\u221a\n\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2\u2225vt\nft\u2212v\u22c6\u22252 (A.55)\n\u2264\u221a\n\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2(\u2206 +\u03f5\u2225v\u22c6\u22252\nc)\n(A.56)\nNext, we bound\u2225B\u22a4\n0v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252:\n\u2225B\u22a4\n0v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252\u2264\u2225B\u22a4\n0v\u22c6\u2212B\u22a4\n\u22c6v\u22c6\u22252+\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252 (A.57)\n\u2264\u03c3max(B0\u2212B\u22c6)\u2225v\u22c6\u22252+\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252 (A.58)\n\u2264\u03f5\u2225v\u22c6\u22252+\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252 (A.59)\n\u2264\u03f5\u2225v\u22c6\u22252+\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252+\u2225Bt\nft\u22a4vt\nft\u2212Bt\nft\u22a4v\u22c6\u22252 (A.60)\n\u2264\u03f5\u2225v\u22c6\u22252+\u2206 +\u221a\n\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2(\u2206 +\u03f5\u2225v\u22c6\u22252\nc)\n(A.61)\n=: \u2206 2 (A.62)\nFinally, we bound|\u2225B\u22a4\n0v\u22c6\u22252\n2\u2212\u2225Bt\nft\u22a4v\u22c6\u22252\n2|, using the identity:\n|\u2225u\u22252\n2\u2212\u2225v\u22252\n2|=|(u\u2212v)\u22a4(u+v)| (A.63)\n\u2264\u2225u\u2212v\u22252\u2225u+v\u22252 (A.64)\n\u2264\u2225u\u2212v\u22252(2\u2225u\u22252+\u2225u\u2212v\u22252) (A.65)\n28",
    "Applying this:\n|\u2225B\u22a4\n0v\u22c6\u22252\n2\u2212\u2225Bt\nft\u22a4v\u22c6\u22252\n2|\u2264\u2225B\u22a4\n0v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252(2\u2225B\u22a4\n0v\u22c6\u22252+\u2225B\u22a4\n0v\u22c6\u2212Bt\nft\u22a4v\u22c6\u22252) (A.66)\n\u2264\u22062(2\u2225B\u22a4\n0v\u22c6\u22252+\u2206 2) (A.67)\n\u2264\u22062(2\u2225B\u22a4\n\u22c6v\u22c6\u22252+2\u2225B\u22a4\n0v\u22c6\u2212B\u22a4\n\u22c6v\u22c6\u22252+\u2206 2) (A.68)\n\u2264\u22062(2\u2225w\u22c6\u22252+2\u03f5\u2225v\u22c6\u22252+\u2206 2) (A.69)\n=: \u2206 3 (A.70)\nStep 3: Use Proposition A.4 to show v0andv\u22c6must be close : The key insight is that we start from\nProposition A.4, and left and right multiply by v\u22c6, after that we use the previous steps and do some some\nstandard perturbation analysis.\nWe start from Proposition A.4:\nv0v\u22a4\n0\u2212B0B\u22a4\n0=vt\nftvt\nft\u22a4\u2212Bt\nftBt\nft\u22a4(A.71)\nThe key step is to left multiply both sides by v\u22a4\n\u22c6and right multiply both sides by v\u22c6to get:\n(v\u22a4\n0v\u22c6)2\u2212\u2225B\u22a4\n0v\u22c6\u22252\n2= (vt\nft\u22a4v\u22c6)2\u2212\u2225Bt\nft\u22a4v\u22c6\u22252\n2 (A.72)\nRearranging, and then using Equation A.66, we get:\n|(vt\nft\u22a4v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|=|\u2225Bt\nft\u22a4v\u22c6\u22252\n2\u2212\u2225B\u22a4\n0v\u22c6\u22252\n2|\u2264\u22063 (A.73)\nThis is close to what we want, except we have (vt\nft\u22a4v\u22c6)2on the LHS instead of (v\u22a4\n\u22c6v\u22c6)2. We previously\nshowed that vt\nftandv\u22c6are close, in Step 1, so with some algebra we can bound the difference between\n(vt\nft\u22a4v\u22c6)2and(v\u22a4\n\u22c6v\u22c6)2:\n|(vt\nft\u22a4v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|=|(vt\nft\u22a4v\u22c6\u2212v\u22a4\n\u22c6v\u22c6)\u22a4(vt\nft\u22a4v\u22c6+v\u22a4\n\u22c6v\u22c6)| (A.74)\n=|(vt\nft\u22a4v\u22c6\u2212v\u22a4\n\u22c6v\u22c6)\u22a4[2v\u22a4\n\u22c6v\u22c6+ (vt\nft\u22a4v\u22c6\u2212v\u22a4\n\u22c6v\u22c6)]| (A.75)\n=|(v\u22a4\n\u22c6(vt\nft\u2212v\u22c6))\u22a4[2v\u22a4\n\u22c6v\u22c6+ (v\u22a4\n\u22c6(vt\nft\u2212v\u22c6))]| (A.76)\n\u2264\u2225vt\nft\u2212v\u22c6\u22252\u2225v\u22c6\u22252\n2[2\u2225v\u22c6\u22252+\u2225vt\nft\u2212v\u22c6\u22252] (A.77)\n= (\u2206/c)\u2225v\u22c6\u22252\n2(2\u2225v\u22c6\u22252+(\u2206/c)) := \u2206 4 (A.78)\nAbove, from the third line to the fourth line, we used triangle inequality and Cauchy-Schwarz.\nSo \ufb01nally, by triangle-inequality we can now bound |(v\u22a4\n\u22c6v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|:\n|(v\u22a4\n\u22c6v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|\u2264|(v\u22a4\n\u22c6v\u22c6)2\u2212(vt\nft\u22a4v\u22c6)2|+|(vt\nft\u22a4v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2| (A.79)\n\u2264\u22064+ \u2206 3 (A.80)\nWrap up i.e., writing out \u22064+ \u2206 3explicitly : This is basically the bound we want, but we would like to\nexpress \u22063,\u22064in terms of \u2206and\u03f5. Note that this step has no insight, and is just algebra\u2014we include the\ndetails for reference and veri\ufb01ability. We recall:\n\u22064= (\u2206/c)\u2225v\u22c6\u22252\n2(2\u2225v\u22c6\u22252+(\u2206/c)) (A.81)\n\u22063= \u2206 2(2\u2225w\u22c6\u22252+2\u03f5\u2225v\u22c6\u22252+\u2206 2) (A.82)\n\u22062=\u03f5\u2225v\u22c6\u22252+\u2206 +\u221a\n\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2(\u2206 +\u03f5\u2225v\u22c6\u22252\nc)\n(A.83)\n29",
    "SinceB0has orthogonal rows (by assumption), B\u22a4\n0has orthogonal columns, so \u2225w\u22c6\u22252=\u2225B\u22a4\n0v\u22c6\u22252=\n\u2225v\u22c6\u22252. In addition, since B0haskorthogonal rows,\u2225B0\u2225F=\u221a\nk. We also note that\u221a\n\u2225B0\u22252\nF+\u2225v\u22c6\u22252\n2\u2264\n\u2225B0\u2225F+\u2225v\u22c6\u22252=\u221a\nk+\u2225w\u22c6\u22252. Sincec\u22641, we have:\n\u03f5\u2225v\u22c6\u22252+\u2206\u2264(\u2206 +\u03f5\u2225v\u22c6\u22252\nc)\n(A.84)\nSo for \u22062, up to constant factors we can ignore the \u03f5\u2225v\u22c6\u22252+\u2206term\u2014this means we get:\n\u22062\u2264O(\n(\u221a\nk+\u2225w\u22c6\u22252)(\u2206 +\u03f5\u2225w\u22c6\u22252\nc))\n(A.85)\nUsing the fact that\u221a\nk+\u2225w\u22c6\u22252\u2264\u221a\nk(1 +\u2225w\u22c6\u2225)we get:\n\u22062\u2264O(\u221a\nk(1 +\u2225w\u22c6\u2225)(\u2206 +\u03f5\u2225w\u22c6\u22252\nc))\n(A.86)\nThen since \u2206 +\u03f5\u2225w\u22c6\u22252\u2264(1 +\u2225w\u22c6\u22252)(\u2206 +\u03f5), we get:\n\u22062\u2264O(\u221a\nk(1 +\u2225w\u22c6\u2225)2(\u2206 +\u03f5\nc))\n(A.87)\nNow for \u22063, \ufb01rst note that \u03f5\u22642, sinceB\u22c6andB0have orthogonormal rows so \u2225B\u22c6\u2212B0\u22252\u22642. This means\nthat\u03f5\u2225w\u22c6\u22252\u2264\u2225w\u22c6\u22252, so\u22063simpli\ufb01es to:\n\u22063\u2264O(\u22062(\u2225w\u22c6\u22252+\u2206 2)) =O(\u22062\u2225w\u22c6\u22252+\u2206 2) (A.88)\nSubstituting the bound for \u22062into\u22063, we get:\n\u22063\u2264O(\u221a\nk\u2225w\u22c6\u22252(1 +\u2225w\u22c6\u2225)2(\u2206 +\u03f5\u2225w\u22c6\u22252\nc)\n+k(1 +\u2225w\u22c6\u2225)4(\u2206 +\u03f5\u2225w\u22c6\u22252\nc)2)\n(A.89)\nFor\u22064, we get:\n\u22064\u2264O(\n\u2225w\u22c6\u22253\n2\u2206\nc+\u2225w\u22c6\u22252(\u2206\nc))\n(A.90)\nSince \u2206/c\u2264(\u2206 +\u03f5)/cand\u2225w\u22c6\u22252\n2\u2264(1 +\u2225w\u22c6\u22252)2we have for the \ufb01nal error \u22063+ \u2206 4:\n\u22063+ \u2206 4\u2264\u221a\nkw(1 +\u2225w\u22c6\u22252\n2)2(\u2206 +\u03f5\nc)\n+k(1 +\u2225w\u22c6\u22252\n2)4(\u2206 +\u03f5\nc)2\n(A.91)\nWrap up i.e., taking the contrapositive: So we\u2019ve shown that if \u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252\n2\u2264\u2206, then:\n|(v\u22a4\n\u22c6v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|\u2264\u2206 +\u03f5\ncw(1 +\u2225w\u22c6\u22252\n2)2\u221a\nk+(\u2206 +\u03f5)2\nc2(1 +\u2225w\u22c6\u22252\n2)4k (A.92)\nWe\u2019d like to \ufb02ip this around: suppose |(v\u22a4\n\u22c6v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|\u2265\u03d52for some\u03d5. To lower bound\u2225B\u22a4\n\u22c6v\u22c6\u2212\nBt\nft\u22a4vt\nft\u22252\n2, we simply take the contrapositive of what we have proved. Let \u2206be given by:\n\u2206 = min(c\nw(1 +\u2225w\u22c6\u22252\n2)2\u221a\nk\u03d52,c\u221a\n(1 +\u2225w\u22c6\u22252\n2)4k\u03d5)\n\u2212\u03f5 (A.93)\nIn this case with some algebra, we can show that:\n|(v\u22a4\n\u22c6v\u22c6)2\u2212(v\u22a4\n0v\u22c6)2|\u2265\u03d52\u2265\u2206 +\u03f5\ncw(1 +\u2225w\u22c6\u22252\n2)2\u221a\nk+(\u2206 +\u03f5)2\nc2(1 +\u2225w\u22c6\u22252\n2)4k (A.94)\n30",
    "To see this, we bound each of the terms in the RHS separately using our de\ufb01nition of \u2206. Then, from the\ncontrapositive of what we proved (compare with Equation A.92, we get:\n\u2225B\u22a4\n\u22c6v\u22c6\u2212Bt\nft\u22a4vt\nft\u22252\n2\u2265\u2206 (A.95)\nFinally, we can massage \u2206to combine terms and make it look slightly nicer:\n\u2206\u2265c\u221a\nkmin(\u03d5,\u03d52/\u2225w\u22c6\u22252)\n(1 +\u2225w\u22c6\u22252)2\u2212\u03f5 (A.96)\nThen applying Lemma A.7 we get the desired result. For even more interpretability, if \u2225w\u22252= 1and\u03d5is\nbounded above by some constant, then you can think of \u2206as approximatelyc\u221a\nk\u03d52\u2212\u03f5. This completes the\nproof.\nA.3 LP vs. FT (OOD)\nWe now prove Theorem 3.5, which compares linear probing and \ufb01ne-tuning in the linear overparameterized\nsetting, when the ID data lies in a lower dimensional subspace.\nWe \ufb01rst state a more precise version of Theorem 3.5\u2014basically we \ufb01x all problem parameters except B0\n(which limits to B\u22c6). To de\ufb01ne the limit, we consider a sequence of pretrained feature extractors: {Bi\n0}\u221e\ni=1.\nWe de\ufb01ne the corresponding limit points of \ufb01ne-tuning and linear probing when we start from the i-th\npretrained feature extractor. That is, let vfti(t),Bfti(t)denote the parameters at time tof \ufb01ne-tuning if\nwe initialize with v0,Bi\n0(see Equation 3.2 for the \ufb01ne-tuning updates). Let v\u221e\nlpi,Bi\n0be the linear probing\nsolution when initialized with v0,Bi\n0(see Equation 3.5 for the linear probing updates). We note that the LP\niterates converge to v\u221e\nlpi,Bi\n0as a result of gradient \ufb02ow on a convex problem.\nFinally, Theorem 3.5 says that as the pretrained representations get better, linear probing does much better\nthan \ufb01ne-tuning OOD:\nTheorem A.8 (Formal statement of Theorem 3.5) .In the linear overparameterized setting, under the ID\nsubspace assumption, \ufb01x the dimensions of the setting d,k,m , number of examples n, the ID subspace S, ID\ndistributionPid, the distribution over the head v0, and the ground truth parameters v\u22c6,B\u22c6. Assume the non-\ndegeneracy conditions cos\u03b8max(R\u2217,S)>0andcos\u03b8max(R\u2217,S\u22a5)>0whereR\u2217=rowspace (B\u22c6). Given\na sequence of pretrained feature extractors {Bi\n0}\u221e\ni=1withBi\n0\u2192B\u22c6, where the limit is in the pseudometric\ngiven by De\ufb01nition 3.1, the ratio of OOD errors of linear probing and \ufb01ne-tuning converges in probability to\n0:\nLood(v\u221e\nlpi,Bi\n0)\ninft\u22650Lood(vfti(t),Bfti(t))p\u21920,asi\u2192\u221e. (A.97)\nThe purpose of the in\ufb01mum is to capture the fact that the bound holds for all times tfor \ufb01ne-tuning (and\ntherefore also for the limit v\u221e\nft,B\u221e\nftwhen it exists). Note that the ratio is a random variable because the\ntraining data is sampled from Pidand the head is sampled ( v0\u223cN(0,\u03c32I)for some\u03c32).\nProof. Recall that we say a sequence of real-valued random variables converges in probability to 0(written\nasXip\u21920) if for every \u03f5\u2032,\u03b4> 0, for all large enough i(that is, for all i\u2265Nifor someNi), we have:\nP(|Xi|>\u03f5\u2032)\u2264\u03b4. (A.98)\n31",
    "Accordingly, \ufb01x arbitrary \u03f5\u2032,\u03b4> 0, and we will show that the ratio of errors is eventually smaller than \u03f5\u2032with\nprobability at least 1\u2212\u03b4.\nLower bounding \ufb01ne-tuning error : SinceBi\n0\u2192B\u22c6, from Lemma A.10 we have that cos\u03b8max(Ri,S\u22a5)\u2192\ncos\u03b8max(R\u2217,S\u22a5)whereRi=rowspace (Bi\n0). Since cos\u03b8max(R\u2217,S\u22a5)>0, this means that for all large\nenoughiwe have:\ncos\u03b8max(Ri,S\u22a5)>cos\u03b8max(R\u2217,S\u22a5)/2. (A.99)\nNext, from Lemma A.12, we have that with probability at least 1\u2212\u03b4/2,Head-Error (v0,v\u22c6) =|(v\u22a4\n0v\u22c6)2\u2212\n(v\u22a4\n\u22c6v\u22c6)2|\u2265c\u03b4for somec\u03b4>0. Plugging this into the \ufb01ne-tuning bound in Theorem 3.3, this means that for\nall large enough iwith probability at least 1\u2212\u03b4/2:\ninf\nt\u22650\u221a\nLood(vfti(t),Bfti(t))\u2265c\u2032\n\u03b4\u2212d(Bi\n0,B\u22c6), (A.100)\nfor somec\u2032\n\u03b4>0. But sinceBi\n0\u2192B\u22c6we haved(Bi\n0,B\u22c6)\u21920asi\u2192\u221e . So this means that for all large\nenoughiwith probability at least 1\u2212\u03b4/2:\ninf\nt\u22650Lood(vfti(t),Bfti(t))\u2265c\u2032\u2032\n\u03b4, (A.101)\nfor somec\u2032\u2032\n\u03b4>0.\nUpper bounding the linear probing error : SinceBi\n0\u2192B\u22c6, from Lemma A.10 we have that cos\u03b8max(Ri,S)\u2192\ncos\u03b8max(R\u2217,S)and so since cos\u03b8max(R\u2217,S)>0, for all large enough iwe have:\ncos\u03b8max(Ri,S)>cos\u03b8max(R\u2217,S)/2. (A.102)\nPlugging this into the RHS of Lemma A.14, Equation A.132, which upper bounds the OOD error of linear\nprobing, we get that for all large enough i, with probability at least 1\u2212\u03b4/2:\nLood(v\u221e\nlpi,Bi\n0)\u2264u\u03b4(d(Bi\n0,B\u22c6))2, (A.103)\nfor someu\u03b4>0. Again since d(Bi\n0,B\u22c6)\u21920asi\u2192\u221e , this means for all large enough i, with probability\nat least 1\u2212\u03b4/2,d(Bi\n0,B\u22c6)will be small enough so that:\nLood(v\u221e\nlpi,Bi\n0)\u2264c\u2032\u2032\n\u03b4\u03f5. (A.104)\nTaking the ratio : So taking the ratio of the lower bound for \ufb01ne-tuning, and upper bound for linear probing,\nwe get with with probability at least 1\u2212\u03b4:\nLood(v\u221e\nlpi,Bi\n0)\ninft\u22650Lood(vfti(t),Bfti(t))\u2264\u03f5, (A.105)\nas desired.\nWe now prove the Lemmas that we used in the above proof.\n32",
    "A.3.1 Convergence of principal angle\nTheorem 3.5 assumes conditions on the angle between the perfect feature extractor B\u22c6and the ID subspace\nS. However, \ufb01ne-tuning and linear probing start from features B0with some error, and do not get access\ntoB\u22c6. We show that if B0andB\u22c6are close, then the angles between their rowspaces to a third subspace T\n(which could be the the ID subspace S) is similar.\nLemma A.9. Given two feature extractors B0,B\u22c6\u2208Rk\u00d7dwith orthonormal rows, where R0=rowspace (B0),R\u2217=\nrowspace (B\u22c6), and a subspace Twith dimension at least 1, we have:\n|cos\u03b8max(R0,T)\u2212cos\u03b8max(R\u2217,T)|\u2264d(B0,B\u22c6) (A.106)\nProof. Recall thatk= dim(R0) = dim(R\u2217). Letr= min(k,dim(T))and letFbe ad-by-dim(T)matrix\nwith orthonormal columns that form a basis for T. We have, for arbitrary rotation matrix U\u2208Rk\u00d7k:\ncos\u03b8max(R0,T) =\u03c3r(B0F) (A.107)\n=\u03c3r(UB0F) (A.108)\n=\u03c3r(B\u22c6F+ (UB0\u2212B\u22c6)F) (A.109)\n\u2265\u03c3r(B\u22c6F)\u2212\u03c31((UB0\u2212B\u22c6)F) (A.110)\n\u2265\u03c3r(B\u22c6F)\u2212\u03c31(UB0\u2212B\u22c6) (A.111)\n=\u03c3r(B\u22c6F)\u2212\u2225UB0\u2212B\u22c6\u22252 (A.112)\n= cos\u03b8max(R\u2217,T)\u2212\u2225UB0\u2212B\u22c6\u22252 (A.113)\nHere in the \ufb01rst step we used the de\ufb01nition of cos\u03b8max(De\ufb01nition 3.2), and the fact that B\u22a4\n0has orthonormal\ncolumns which form a basis for R0(the rowspace of B0), so in De\ufb01nition 3.2 we can subtitute E=B\u22a4\n0.\nTo get Equation A.110 we used Weyl\u2019s theorem, which bounds the singular value under perturbations:\n\u03c3r(A+B)\u2265\u03c3r(A)\u2212\u03c31(B). To get Equation A.111 we used the fact that \u2225Fv\u22252=\u2225v\u2225sinceFhas\northonormal columns.\nSince this holds for all rotation matrices U, we can take the minimum over Uto get:\ncos\u03b8max(R0,T)\u2265cos\u03b8max(R\u2217,T)\u2212min\nU\u2225UB0\u2212B\u22c6\u22252= cos\u03b8max(R\u2217,T)\u2212d(Bi\n0,B\u22c6) (A.114)\nSince the relationship between B0andB\u22c6are symmetric (and the distance dis symmetric), this gives us the\ndesired result:\n|cos\u03b8max(R0,T)\u2212cos\u03b8max(R\u2217,T)|\u2264d(B0,B\u22c6) (A.115)\nLemma A.10. Given a sequence of pretrained feature extractors {Bi\n0}\u221e\ni=1withBi\n0\u2192B\u22c6, whereBi\n0,B\u22c6\u2208\nRk\u00d7dhave orthonormal rows, let Ri=rowspace (Bi\n0),R\u2217=rowspace (B\u22c6). Then for any subspace T, we\nhave:\ncos\u03b8max(Ri,T)\u2192cos\u03b8max(R\u2217,T),asi\u2192\u221e. (A.116)\nProof. This follows directly from Lemma A.9. Bi\n0\u2192B\u22c6meansd(Bi\n0,B\u22c6)\u21920. Then from Lemma A.9:\n|cos\u03b8max(Ri,T)\u2212cos\u03b8max(R\u2217,T)|\u21920,asi\u2192\u221e (A.117)\nThis means cos\u03b8max(Ri,T)\u2192cos\u03b8max(R\u2217,T)asi\u2192\u221e\n33",
    "A.3.2 Bounding the head error\nWe prove a lower bound on Head-Error (v0,v\u22c6) =|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|, which was a key term in the \ufb01ne-\ntuning lower bound (Theorem 3.3). Note that if the head is initialized as v0= 0, then Head-Error (v0,v\u22c6) =\n\u2225v\u22c6\u22252\n2=\u2225w\u22c6\u22252\n2. In practice, the head is usually initialized randomly, for example normally distributed.\nIntuitively, the head error is still high because we do not know which direction the head is pointing in, so most\nof the time the initial (randomly sampled) head will be pointing in the wrong direction. If v0\u223cN(0,\u03c32I)\ncan show that for any \u03c32, the head error will still typically be at least \u2126(\u2225v\u22c6\u22252)This is an illustrative result,\none can show similar results for other random initializations as well.\nWe \ufb01rst prove an anti-concentration lemma, which says that if uis univariate Gaussian, then it cannot be too\nclose to any particular constant a, no matter how the variance of the Gaussian is chosen.\nLemma A.11. For some universal constant c, givena>0, for all\u03bd2ifu\u223cN(0,\u03bd2)then for all 0\u2264\u03b4\u22641:\nP(|u\u2212a|\u2264c\u03b4a)\u2264\u03b4 (A.118)\nProof. Consider\u03b4such that\u03b4\u22641/10. Then for all uwith|u\u2212a|\u2264\u03b4a, we haveu\u22659a/10. For all\nu\u22659a/10, the density f(u)is upper bounded (from the formula for the density of a Gaussian random\nvariable) by:\nf(u)\u2264O(1\nvexp\u221292a2\n2\u00b7102v2) (A.119)\nWe can maximize this explicitly (e.g., use Mathematica or by taking the logarithm and then setting the\nderivative to 0) and we get for some universal constant c\u2032\u226510(it is OK to choose a larger universal constant\nthan needed):\nf(u)\u2264c\u2032\na(A.120)\nSince the density is less than c\u2032/aand if|u\u2212a|\u2264\u03b4athe size of the interval is 2\u03b4a, we get for all \u03b4\u22641/10:\nP(|u\u2212a|\u2264\u03b4a)\u22642c\u2032\u03b4a\na= 2c\u2032\u03b4 (A.121)\nNow, we substitute \u03b4\u2032= 2c\u2032\u03b4. We get for all \u03b4\u2032\u22642c\u2032/10:\nP(|u\u2212a|\u22641\n2c\u2032\u03b4\u2032a)\u2264\u03b4\u2032(A.122)\nSincec\u2032\u226510,2c\u2032/10\u22651, so the statement is true for all 0\u2264\u03b4\u2032\u22641.\nWe now bound the error in the head if the initialization is Gaussian. This bound holds for all initialization\nvariances\u03c32. Similar bounds can be shown for other (non-Gaussian) head initializations using similar\nanti-concentration arguments.\nLemma A.12. For some universal constant c, for allv\u22c6\u2208Rkwithv\u22c6\u0338= 0,\u03c3\u2208R+,\u03b4\u2208[0,1], if\nv0\u223cN(0,\u03c32Ik), we have with probability at least 1\u2212\u03b4:\n(Head-Error (v0,v\u22c6))2:=|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|\u2265c\u03b4(v\u22a4\n\u22c6v\u22c6)2(A.123)\n34",
    "Proof. First note that Head-Error (v0,v\u22c6) =Head-Error (\u2212v0,v\u22c6)andv0is symmetric around 0(v0and\u2212v0\nhave the same probability), and is almost surely not exactly 0. So without loss of generality, we can suppose\nthatv\u22a4\n0v\u22c6\u22650.\nSuf\ufb01ces to bound |v\u22a4\n0v\u22c6\u2212v\u22a4\n\u22c6v\u22c6|: We decompose the error:\n|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|=|v\u22a4\n0v\u22c6\u2212v\u22a4\n\u22c6v\u22c6|(|v\u22a4\n0v\u22c6+v\u22a4\n\u22c6v\u22c6|) (A.124)\n\u2265|v\u22a4\n0v\u22c6\u2212v\u22a4\n\u22c6v\u22c6|(v\u22a4\n\u22c6v\u22c6)| (A.125)\nSo we bound|v\u22a4\n0v\u22c6\u2212v\u22a4\n\u22c6v\u22c6|.\nv\u22a4\n0v\u22c6is normally distributed : We note that v\u22a4\n0v\u22c6is distributed as:\nv\u22a4\n0v\u22c6\u223cN(0,\u03c32v\u22a4\n\u22c6v\u22c6) (A.126)\nIn other words, a normal with mean 0, and variance\u03c32\n1=\u03c32v\u22a4\n\u22c6v\u22c6, and therefore standard deviation\n\u03c31=\u03c3\u221a\nv\u22a4\u22c6v\u22c6.\nApply Gaussian anti-concentration lemma : Then, from Lemma A.11, we have for some universal constant\ncthat with probability at least 1\u2212\u03b4:\n|v\u22a4\n0v\u22c6\u2212v\u22a4\n\u22c6v\u22c6|\u2265c\u03b4v\u22a4\n\u22c6v\u22c6 (A.127)\nSo substituting this back into Equation A.124, we get the desired result:\n|(v\u22a4\n0v\u22c6)2\u2212(v\u22a4\n\u22c6v\u22c6)2|\u2265c\u03b4(v\u22a4\n\u22c6v\u22c6)2(A.128)\nA.3.3 Upper bounding linear probing error\nWe showed a lower bound for the OOD error of \ufb01ne-tuning in Theorem 3.3. To compare this with linear\nprobing, we prove an upper bound on the OOD error of linear probing.\nFor completeness we include an elementary lemma (note that the condition that the matrices are tall is\nimportant for composing \u03c3min, unlike for \u03c3max, and we included this lemma to be careful about these\nconditions):\nLemma A.13. Suppose we have two matrices A,Bof shape (r,s)and(s,t)respectively, and they are tall\nmatrices sor\u2265s\u2265t. Then we have:\n\u03c3min(AB)\u2265\u03c3min(A)\u03c3min(B) (A.129)\nProof. For a tall matrix A, we have:\n\u03c3min(A) = min\n\u2225x\u22252\u22641\u2225Ax\u22252 (A.130)\nSo we have:\n\u03c3min(AB) = min\n\u2225x\u22252\u22641\u2225ABx\u22252\u2265\u03c3min(A)\u03c3min(B) min\n\u2225x\u22252\u22641\u2225x\u22252 (A.131)\nAndmin\u2225x\u22252\u22641\u2225x\u22252= 1which completes the proof.\n35",
    "Lemma A.14. In the linear overparameterized setting, under the ID subspace assumption, \ufb01x arbitrary Pz.\nThen there exists c\u03b4such that with probability at least 1\u2212\u03b4, for alld,n,m,k,w \u22c6, feature extractors B\u22c6,B0,\nand ID subspaces Swith corresponding F(whose columns are orthonormal and form a basis for S), if\ncos\u03b8max(S,R)>0, we have:\n\u221a\nLood(v\u221e\nlp,B0)\u2264(c\u03b4\ncos\u03b8max(S,R))2\nd(B0,B\u22c6)||w\u2217||2 (A.132)\nIfPzis isotropic Gaussian so N(0,Im), then we derive a bound for c\u03b4analytically: if n\u22655mand\nn\u226510 log1\n\u03b4then with probability at least 1\u2212\u03b4, the linear probing OOD error is upper bounded by:\n\u221a\nLood(v\u221e\nlp,B0)\u2264O(log(n/\u03b4)\n(cos\u03b8max(R,S))2d(B0,B\u22c6)\u2225w\u22c6\u22252)\n(A.133)\nProof. From the ID subspace assumption, the data matrix Xof shape (n,d)can be written as X=ZF\u22a4\nwhereZbe a matrix of shape (n,m)with each row Zisampled iid from Pz, andFis a matrix of shape\n(d,m)whose columns are orthonormal and form a basis for the ID subspace S.\nLet\u03f5=\u2225B\u22c6\u2212B0\u22252\u2264. We \ufb01rst prove the bounds for \u03f5, in terms of d(B0,B\u22c6)and we later handle the fact that\nthe feature extractor distance involves the min over rotation matrices U:d(B0,B\u22c6) = minU\u2225UB0\u2212B\u22c6\u22252.\nBounding key singular values : Before proceeding with the proof, we examine a key quantity XB\u22a4\n0=\nZF\u22a4B\u22a4\n0which comes up in the Hessian of the loss function. We will show that this is invertible almost\nsurely, and get a lower bound on its min singular value.\nFirst, we examine the shapes of the matrices. ZF\u22a4B\u22a4\n0has shape (n,d)whereZhas shape (n,m)and\nF\u22a4B\u22a4\n0has shape (m,k). Sincen\u2265m > k we have that ZandF\u22a4B\u22a4\n0are tall matrices, and so from\nLemma A.13 we can write the min singular value of ZF\u22a4B\u22a4\n0as:\n\u03c3min(ZF\u22a4B\u22a4\n0)\u2265\u03c3min(Z)\u03c3min(F\u22a4B\u22a4\n0) (A.134)\nNow from the de\ufb01nion of the principal angle (De\ufb01nition 3.2), we have:\n\u03c3min(F\u22a4B\u22a4\n0) = cos\u03b8max(R,S)>0. (A.135)\nSince we assumed Pzhas density in the ID subspace assumption, from Lemma 3 in Xie et al. (2021a) we get\nthat for some c\u2032\n\u03b4>0that depends on \u03b4andPz, with probability at least 1\u2212\u03b4:\n\u03c3min(Z)\u2265c\u2032\n\u03b4 (A.136)\nNote that this also means that \u03c3min(ZF\u22a4B\u22a4\n0)>0and soXB\u22a4\n0=ZF\u22a4B\u22a4\n0has full rank kalmost surely.\nThis also implies that B0X\u22a4XB\u22a4\n0is a matrix of shape (k,k)that is invertible almost surely.\nMain proof SinceB0X\u22a4XB\u22a4\n0is invertible almost surely, there is a unique global minimum (minimizing\noverv) to the loss optimized by linear-probing:\narg min\nv\u2225XB\u22a4\n0v\u2212XB\u22a4\n\u22c6v\u22c6\u22252\n2= (B0X\u22a4XB\u22a4\n0)\u22121B0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.137)\nWe can see this by noting that the loss function on the LHS is strongly convex in vsince the Hessian\nB0X\u22a4XB\u22a4\n0is invertible. Then, gradient \ufb02ow converges to the unique minimizer on the RHS, so:\nv\u221e\nlp= (B0X\u22a4XB\u22a4\n0)\u22121B0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.138)\n36",
    "We now bound the square-root OOD error (taking the square root makes it easier to apply triangle inequalities),\nstarting with the de\ufb01nition:\n\u221a\nLood(v\u221e\nlp,B0) =\u2225B\u22a4\n\u22c6v\u22c6\u2212B\u22a4\n0v\u221e\nlp\u22252 (A.139)\n\u2264\u2225(B\u22a4\n\u22c6v\u22c6\u2212B\u22a4\n0v\u22c6) + (B\u22a4\n0v\u22c6\u2212B\u22a4\n0v\u221e\nlp)\u22252 (A.140)\n\u2264\u2225B\u22a4\n\u22c6v\u22c6\u2212B\u22a4\n0v\u22c6\u22252\ued19\ued18\ued17\ued1a\n(1)+\u2225B\u22a4\n0v\u22c6\u2212B\u22a4\n0v\u221e\nlp)\u22252\ued19\ued18\ued17\ued1a\n(2)(A.141)\nWe bound each term on the RHS of the last line. For term (1):\n\u2225B\u22a4\n\u22c6v\u22c6\u2212B\u22a4\n0v\u22c6\u22252\u2264\u03c3max(B\u22c6\u2212B0)\u2225v\u22c6\u22252 (A.142)\n\u2264\u03f5\u2225v\u22c6\u22252 (A.143)\n=\u03f5\u2225w\u22c6\u22252. (A.144)\nWhere we note that \u2225v\u22c6\u22252=\u2225w\u22c6\u22252becausew\u22c6=B\u22a4\n\u22c6v\u22c6where the rows of B\u22c6(columns of B\u22a4\n\u22c6) are\northonormal.\nLet\u03a3 =X\u22a4X. For term (2), we \ufb01rst subtitute v\u221e\nlpand do some algebra (again noting that \u2225v\u22c6\u22252=\u2225w\u22c6\u22252)\nto get:\n\u2225B\u22a4\n0v\u22c6\u2212B\u22a4\n0v\u221e\nlp\u22252=\u2225B\u22a4\n0(B0\u03a3B\u22a4\n0)\u22121B0\u03a3B\u22a4\n0v\u22c6\u2212B\u22a4\n0v\u221e\nlp\u22252 (A.145)\n=\u2225B\u22a4\n0(B0\u03a3B\u22a4\n0)\u22121B0\u03a3(B0\u2212B\u22c6)\u22a4v\u22c6\u22252 (A.146)\n\u2264\u03c3max(B\u22a4\n0(B0\u03a3B\u22a4\n0)\u22121B0\u03a3)\u03c3max(B0\u2212B\u22c6)\u2225w\u22c6\u22252 (A.147)\n\u2264\u03c3max(B\u22a4\n0(B0\u03a3B\u22a4\n0)\u22121B0\u03a3)\u03f5\u2225w\u22c6\u22252 (A.148)\n\u2264\u03c3max(B0)2\u03c3max(\u03a3)1\n\u03c3min(B0\u03a3B\u22a4\n0)\u03f5\u2225w\u22c6\u22252 (A.149)\n\u2264\u03c3max(B0)2\u03c3max(X)2\n\u03c3min(XB\u22a4\n0)2\u03f5\u2225w\u22c6\u22252 (A.150)\n=\u03c3max(B0)2\u03c3max(ZF\u22a4)2\n\u03c3min(ZF\u22a4B\u22a4\n0)2\u03f5\u2225w\u22c6\u22252 (A.151)\n\u2264\u03c3max(B0)2\u03c3max(Z)2\n\u03c3min(Z)2(cos\u03b8max(R,S))2\u03f5\u2225w\u22c6\u22252 (A.152)\n(A.153)\nWhere in the \ufb01rst line we subtituted in the closed form for v\u221e\nlpfrom Equation A.137, and in the last line\nwe used the fact that \u03c3max(ZF\u22a4)\u2264\u03c3max(Z)sinceF\u22a4has orthonormal rows, and \u03c3min(ZF\u22a4B\u22a4) =\n\u03c3min(Z) cos\u03b8max(R,S)as explained in Equation A.134 and Equation A.135.\nSo it suf\ufb01ces to bound the quantities in the RHS. Since B0has orthonormal rows, \u03c3max(B0) = 1 .\nNo Gaussian assumption : For the \ufb01rst part of the Theorem (Equation A.132 where we make no Gaussian\nassumptions, but give a less quantitative bound), we just use the fact that \u03c3max(Z)is upper bounded almost\nsurely, and\u03c3min(Z)\u2265c\u2032\n\u03b4with probability at least 1\u2212\u03b4. This implies that for some c\u03b4>0with probability\nat least 1\u2212\u03b4:\u221a\nLood(v\u221e\nlp,B0)\u2264(c\u03b4\ncos\u03b8max(S,R))2\n\u03f5||w\u2217||2, (A.154)\n37",
    "where\u03f5=\u2225B0\u2212B\u22c6\u22252.\nGaussian assumption : For the second part of the Theorem (Equation A.133 where we assume Pzis\nGaussian), we use results in random matrix theory to lower bound and upper bound \u03c3min(Z). For the lower\nbound we use a result from Rudelson & Vershynin (2009) (see page 4, in the equation below Equation 1.11),\nsinceZ\u2208Rn\u00d7mis a matrix with each entry sampled from N(0,1), we get for all t>0:\nP(\u03c3min(Z)\u2264\u221an\u2212\u221am\u2212t)\u2264e\u2212t2/2(A.155)\nWith a bit of algebra, this gives us that with probability at least 1\u2212\u03b4:\n\u03c3min(Z)\u2265\u221an\u2212\u221am\u2212\u221a\n2 log1\n\u03b4(A.156)\nWe assumed n\u22655mandn\u226510 log1\n\u03b4, so we get:\n\u03c3min(Z)\u2265O(\u221an) (A.157)\nThe upper bound is a standard matrix concentration bound\u2014we use the high probability bound in Theorem\n4.1.1 from Tropp (2015) (see Section 4.2.2 which calculates the variance statistic for rectangular Gaussian\nmatrices, also notice the square on the LHS below):\n\u03c3max(Z)2\u2264O(nlogn\n\u03b4) (A.158)\nSubstituting the lower and upper bounds on \u03c3min(Z)into Equation A.145 we get:\n\u2225B\u22a4\n0v\u22c6\u2212B\u22a4\n0v\u221e\nlp\u22252\u2264O(log(n/\u03b4)\n(cos\u03b8max(R,S))2\u03f5\u2225w\u22c6\u22252)\n(A.159)\nSubstituting into equation A.139, we have:\n\u221a\nLood(v\u221e\nlp,B0)\u2264O(log(n/\u03b4)\n(cos\u03b8max(R,S))2\u03f5\u2225w\u22c6\u22252)\n, (A.160)\nwhere\u03f5=\u2225B0\u2212B\u22c6\u22252. Which completes the proof of the second part (Equation A.133).\nHandling the rotation matrix U: We now handle the fact that the feature extractor distance involves the\nmin over rotation matrices U:d(B0,B\u22c6) = minU\u2225UB0\u2212B\u22c6\u22252. Letv\u221e\nlp(B0)denote the linear probing head\nsolution if we use a pretrained feature extractor B0. We \ufb01rst note that for any k-by-krotation matrix U, we\nhave:\nLood(v\u221e\nlp(B0),B0) =Lood(v\u221e\nlp(UB0),UB 0). (A.161)\nThis follows from using the closed form we derived above for v\u221e\nlp(B0)and some simple algebraic manipula-\ntion (e.g., recall that U\u22121=Y\u22a4):\n(UB0)\u22a4v\u221e\nlp(UB0) = (UB0)\u22a4(UB0X\u22a4XB\u22a4\n0U\u22a4)\u22121UB0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.162)\n=B\u22a4\n0U\u22a4U(B0X\u22a4XB\u22a4\n0)\u22121U\u22a4UB0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.163)\n=B\u22a4\n0(U\u22a4U)(B0X\u22a4XB\u22a4\n0)\u22121(U\u22a4U)B0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.164)\n=B\u22a4\n0(B0X\u22a4XB\u22a4\n0)\u22121B0X\u22a4XB\u22a4\n\u22c6v\u22c6 (A.165)\n=B\u22a4\n0v\u221e\nlp(B0) (A.166)\n38",
    "So the \ufb01nal predictors in both cases, (UB0)\u22a4v\u221e\nlp(UB0)andB\u22a4\n0v\u221e\nlp(B0)are identical. This means that the\nOOD errorLood(v,B) =\u2225B\u22a4v\u2212B\u22a4\n\u22c6v\u22c6\u22252is the same in both cases.\nThis means that we can just take the min over all rotation matrices U(where the \ufb01rst step follows since the\nidentity matrix is a rotation matrix, and the second step is from Equation A.154):\nLood(v\u221e\nlp(B0),B0)\u2264min\nULood(v\u221e\nlp(UB0),UB 0) (A.167)\n\u2264min\nU(c(\u03b4)\ncos\u03b8max(S,R))2\n\u2225UB0\u2212B\u22c6\u22252||w\u2217||2\n2 (A.168)\n=(c(\u03b4)\ncos\u03b8max(S,R))2\nd(B0,B\u22c6)||w\u2217||2\n2, (A.169)\nwhich is as desired. We repeat the same thing for Equation A.160 to get Equation A.133 in the Theorem\nstatement.\nA.4 LP vs. FT (OOD), non-asymptotic result for Gaussian covariates\nTheorem 3.5 showed an asymptotic result: if the error d(B0,B\u22c6)\u21920, then linear probing (LP) achieves\nbetter out-of-distribution (OOD) error than \ufb01ne-tuning (FT). Here we give a more quantitative version of\nTheorem 3.5 for Gaussian covariates. The result can be extended to the case there each entry of Pzis\nindependent and identically distributed, mean-zero, constant non-zero variance, but instead of Gaussian\nis sub-Gaussian with constant sub-Gaussian variance / moment\u2014this can be shown using Theorem 1.1\nin Rudelson & Vershynin (2009), which is a different matrix concentration inequality.\nWe show that LP does better than FT out-of-distribution if the error is less than a speci\ufb01c quantity (in terms\nof the representation dimension k, and the angles between the ID subspace Sand the important pretrained\ndirectionsR\u2217=rowspace (B\u22c6)).\nTheorem A.15. In the linear overparameterized setting, under the ID subspace assumption, assume the\nnon-degeneracy conditions cos\u03b8max(R\u2217,S)>0andcos\u03b8max(R\u2217,S\u22a5)>0whereR\u2217=rowspace (B\u22c6).\nSuppose the covariates are generated from a Gaussian distribution on the ID subspace S, soPz=N(0,Im).\nLet\u2225w\u22c6\u22252be a \ufb01xed constant. Given failure probability 1\u2264\u03b4>0, for allw\u22c6,B0,n,d,k,\u03f5 , ifn\u22655m, and\nn\u226510 log1\n\u03b4, if the error of the pretrained representation is not too high:\nd(B0,B\u22c6)<O(cos\u03b8max(R\u2217,S\u22a5)(cos\u03b8max(R\u2217,S))2\u03b42\n\u221a\nklog(n/\u03b4))\n, (A.170)\nthen with probability at least 1\u2212\u03b4, the OOD error of linear probing is lower (better) than for \ufb01ne-tuning at\nall time steps t\u22650in the \ufb01ne-tuning trajectory:\nLood(v\u221e\nlpi,Bi\n0)<inf\nt\u22650Lood(v\u221e\nlpi,Bi\n0). (A.171)\nProof. Let\u03f5=d(B0,B\u22c6). We \ufb01rst note that the condition in Equation A.170 implies that d(B0,B\u22c6)<\nO(cos\u03b8max(R\u2217,S\u22a5))andd(B0,B\u22c6)<O(cos\u03b8max(R\u2217,S)). This is because the cosine angles are between\n0and1,\u03b4is between 0and1, andkandnare at least 1. We now simplify and combine the linear probing\nand \ufb01ne-tuning bounds.\nLetR0=rowspace (B0). Warning: note that the Equation A.170 in the Theorem statement assumes\n39",
    "conditions on the angles between R\u2217(corresponding to the optimal representation) and the ID subspace S.\nHowever, our results that bounded the \ufb01ne-tuning (Theorem 3.3) and linear probing (Lemma A.133) errors\nrequire conditions on the angles between R0(corresponding to the representation that linear probing and\n\ufb01ne-tuning use) and S. So we have to be careful about this distinction, and use Lemma A.9 to relate the two,\nwhich we do below.\nFine-tuning : From Theorem 3.3, we get:\n\u221a\nLood(vft(t),Bft(t))\u2265O(cos\u03b8max(R0,S\u22a5)\u221a\nkmin(\u03d5,\u03d52/\u2225w\u22c6\u22252)\n(1 +\u2225w\u22c6\u22252)2)\n\u2212\u03f5. (A.172)\nWhere\u03d5is the head-error, which we lower bounded in Lemma A.12\u2014subtituting this bound and noting that\nmin(\u03d5,\u03d52) =O(\u03d52),\u2225v\u22c6\u22252=\u2225w\u22c6\u22252(which we assumed is a constant), this gives us:\n\u221a\nLood(vft(t),Bft(t))\u2265O(cos\u03b8max(R0,S\u22a5)\u221a\nk\u03b42)\n\u2212\u03f5 (A.173)\nNow, sinced(B0,B\u22c6) =\u03f5, we use Lemma A.9 to get that:\ncos\u03b8max(R0,S\u22a5)\u2265cos\u03b8max(R\u2217,S\u22a5)\u2212\u03f5 (A.174)\nSubtituting this into Equation A.173, we get (notice the R\u2217instead ofR0below):\n\u221a\nLood(vft(t),Bft(t))\u2265O(cos\u03b8max(R\u2217,S\u22a5)\u2212\u03f5\u221a\nk\u03b42)\n\u2212\u03f5 (A.175)\nSince\u03f5\u2264O(cos\u03b8max(R\u2217,S\u22a5)), this can be simpli\ufb01ed to:\n\u221a\nLood(vft(t),Bft(t))\u2265O(cos\u03b8max(R\u2217,S\u22a5)\u221a\nk\u03b42)\n\u2212\u03f5 (A.176)\nLinear probing : From Lemma A.133, we get:\n\u221a\nLood(v\u221e\nlp,B0)\u2264O(log(n/\u03b4)\n(cos\u03b8max(R0,S))2\u03f5\u2225w\u22c6\u22252)\n(A.177)\nAgain, we use Lemma A.9 to get:\ncos\u03b8max(R0,S)\u2265cos\u03b8max(R\u2217,S)\u2212\u03f5 (A.178)\nSubstituting into Equation A.177, and using the fact that \u03f5\u2264O(cos\u03b8max(R\u2217,S)), and since we assumed\n\u2225w\u22c6\u22252is a constant, we get:\n\u221a\nLood(v\u221e\nlp,B0)\u2264O(log(n/\u03b4)\n(cos\u03b8max(R\u2217,S))2\u03f5)\n(A.179)\nCombining the two : We want to show that the OOD error of LP is less than for \ufb01ne-tuning:\nO(log(n/\u03b4)\n(cos\u03b8max(R\u2217,S))2\u03f5)\n\u2264O(cos\u03b8max(R\u2217,S\u22a5)\u221a\nk\u03b42)\n\u2212\u03f5 (A.180)\nWe can bring the \u03f5to the LHS, so this is equivalent to showing:\nO(log(n/\u03b4)\n(cos\u03b8max(R\u2217,S))2\u03f5)\n+\u03f5\u2264O(cos\u03b8max(R\u2217,S\u22a5)\u221a\nk\u03b42)\n(A.181)\n40",
    "Since log(n/\u03b4)\u22651andcos\u03b8max(R\u2217,S))2is between 0and1, this is equivalent to folding the \u03f5inside the\nbig-oh on the LHS:\nO(log(n/\u03b4)\n(cos\u03b8max(R\u2217,S))2\u03f5\u2225w\u22c6\u22252)\n\u2264O(cos\u03b8max(R\u2217,S\u22a5)\u221a\nk\u03b42)\n(A.182)\nBut assuming the condition on \u03f5in Equation A.170 of the Theorem statement, this is easy to show with a bit\nof algebra.\nA.5 Principal angles are likely non-zero\nIn Theorems 3.3, 3.5, and 3.6, we assumed the cosine of the largest principal angle between the representations\nand ID subspace (or complement of the ID subspace) was non-zero. For example, Theorem 3.5 assumed\nthe largest principal angle between R\u2217=rowspace (B\u22c6)and the ID subspace Sis non-zero, and similarly\nfor the angle between R\u2217andS\u22a5. Having an angle of 0 is a degenerate condition. As an example, look at\nFigure 2\u2014here the input dimension d= 2, the representation dimension k= 1, and the ID subspace Shas\ndimension 1. The only way these angles can be 0 is if B\u22a4\n\u22c6is exactly in the same direction as SorS\u22a5, which\nseems like too much of a coincidence. intuitively, if nature introduces even a small amount of randomness in\neither the optimal representation or ID subspace, the angle will be non-zero.\nThis example was in two dimensions\u2014to make this intuition a bit more formal in higher dimensions, we\nprove a simple claim. Lemma A.16 shows that if the Sis a randomly selected mdimensional subspace, then\nthe angles cos\u03b8max(R\u2217,S)andcos\u03b8max(R\u2217,S\u22a5)are non-zero (and we get quantitative lower bounds on\nthem).\nLemma A.16. LetRbe a \ufb01xedkdimensional subspace, and let Sbe a uniformly random mdimensional\nsubspace (formally, a uniform measure on the Grassmannian manifold) in Rdwithm > k . Then with\nprobability at least 1\u2212\u03b4,\ncos\u03b8max(R,S)\u2265\u221am\u2212\u221a\nk\u2212\u221a\n2 log1\n\u03b4\u221a\ndlog2d\n\u03b4(A.183)\nIn addition, we get that cos\u03b8max(R,S)>0almost surely (with probability 1).\nIfm\u22655kandm\u226510 log1\n\u03b4, then we get with probability at least 1\u2212\u03b4:\ncos\u03b8max(R,S)\u2265O(\u221a\nm\ndlog2d\n\u03b4)\n(A.184)\nRecall that big-oh notation here means that the RHS is true for some universal constant (independent of any\nother problem parameters).\nProof. Note that principal angles are invariant if we rotate RandSby the same rotation matrix U. That is, if\nwe letU\u2208Rd\u00d7dbe a rotation matrix, and E\u2208Rd\u00d7k,F\u2208Rd\u00d7mhave orthonormal columns which form a\nbasis forRandSrespectively, then we have:\ncos\u03b8max(R,S) =\u03c3k(E\u22a4F) =\u03c3k((UE)\u22a4(UF)) (A.185)\n41",
    "This symmetry means that we can \ufb01x Sand instead consider Rto be a uniform random kdimensional\nsubspace on the Grassmannian manifold. Without loss of generality, we can also \ufb01x Sto be the span of the\n\ufb01rstmstandard basis vectors: (e1,...,em), whereei\u2208Rdhas a 1 in the i-th entry and a 0 in every other\nentry.\nEquivalently, let MRbe ad-by-kmatrix, where each column is sampled independently from N(0,Id)\u2014since\nthe columns of MRspan a uniformly random k-dimensional subspace, we can let Rbe range ofMR. This is\nequivalent to sampling each entry of MRfromN(0,1).\nLetc= cos\u03b8max(R,S). From Lemma A.2, ccan be written as:\nc= min\nr\u2208R,\u2225r\u22252=1\u2225F\u22a4r\u22252= min\nr\u2208R,\u2225r\u22252\u22651\u2225F\u22a4r\u22252 (A.186)\nSinceRis the range of MR, anyr\u2208Rcan be written as r=MR\u03bbfor some\u03bb\u2208Rk. We \ufb01rst show that\n\u2225\u03bb\u22252cannot be much smaller than \u2225r\u22252. This is because:\n\u2225r\u22252=\u2225MR\u03bb\u22252\u2264\u03c3max(MR)\u2225\u03bb\u22252 (A.187)\nSo this gives us:\n\u2225\u03bb\u22252\u2265\u2225r\u22252\n\u03c3max(MR)(A.188)\nSo everyr\u2208Rcan be written as MR\u03bbwhere\u2225\u03bb\u22252is lower bounded as above.\nWe now simplify the de\ufb01nition of c, starting from Equation A.186.\nc= min\nr\u2208R,\u2225r\u22252\u22651\u2225F\u22a4r\u22252 (A.189)\n\u2265 min\n\u2225\u03bb\u2225\u22651/\u03c3max(MR)\u2225F\u22a4MR\u03bb\u22252 (A.190)\n\u2265 min\n\u2225\u03bb\u2225\u22651/\u03c3max(MR)\u03c3min(F\u22a4MR)\u2225\u03bb\u22252 (A.191)\n=\u03c3min(F\u22a4MR)\n\u03c3max(MR)(A.192)\nSo now we want to lower bound the ratio of two random matrices. We note that F\u22a4MRis a matrix of size\n(m,k)with each entry sampled independently from N(0,1)(this is because F\u22a4simple selects the \ufb01rst m\nrows ofMR).MRis a matrix of size (d,k)with each entry sampled independently from N(0,1).\nNow, as in the Gaussian assumption step of the proof of Lemma A.14, we can apply standard matrix\nconcentration bounds (page 4, below Equation 1.11, in Rudelson & Vershynin (2009) for the bound on \u03c3min,\nand Theorem 4.1.1 in Tropp (2015) for the bound on \u03c3max). We get that with probability at least 1\u2212\u03b4:\n\u03c3min(F\u22a4MR)\u2265\u221am\u2212\u221a\nk\u2212\u221a\n2 log1\n\u03b4(A.193)\n\u03c3max(MR)\u2264\u221a\ndlog2d\n\u03b4(A.194)\nNote that we can use alternate bounds for \u03c3minin Rudelson & Vershynin (2009) that are sometimes tighter.\n42",
    "For the ratio of the two, we get that with probability at least 1\u2212\u03b4, we have:\nc\u2265\u03c3min(F\u22a4MR)\n\u03c3max(MR)\u2265\u221am\u2212\u221a\nk\u2212\u221a\n2 log2\n\u03b4\u221a\ndlog2d\n\u03b4(A.195)\nFor interpretability, ignoring log factors this is approximately:\nc\u2a86\u221am\u2212\u221a\nk\u221a\nd(A.196)\nThe result when m\u22655kandn\u226510 log2\n\u03b4follows with simple algebra.\nFor the result where we show cos\u03b8max(R,S)>0almost surely, we recall that F\u22a4MRis a matrix of size\n(m,k)with each entry sampled independently from N(0,1). Then applying Lemma 3 in Xie et al. (2021a),\nwe get that\u03c3min(F\u22a4MR)>0almost surely. Since \u03c3max(MR)is \ufb01nite, this gives us cos\u03b8max(R,S)>0\nalmost surely.\nIn our case, the dimension of the ID subspace Sism, and the dimension of R\u2217=rowspace (B\u22c6)isk, with\nk < m andk < d\u2212m. IfSis a uniformly random m-dimensional subspace, then S\u22a5is a uniformly\nrandomd\u2212mdimensional subspace. In this case, Lemma A.16 tells us that cos\u03b8max(R\u2217,S)>0and\ncos\u03b8max(R\u2217,S\u22a5)>0almost surely, and gives us quantitative lower bounds for these angles.\nA.6 LP vs. FT (ID)\nWe prove Proposition 3.6, where we show that if the representation is imperfect, then \ufb01ne-tuning does better\nthan linear probing, in-distribution.\nRestatement of Proposition 3.6. In the linear overparameterized setting, under the ID subspace assump-\ntion (Assumption 3.4), let R0=rowspace (B0), andRaug=Span({w\u22c6}\u222aR0). Supposew\u22c6\u0338\u2208R0,\ncos\u03b8max(S,R aug)\u0338= 0, and that \ufb01ne-tuning converges to a local minimum of its loss, then \ufb01ne-tuning does\nbetter ID almost surely: Lid(v\u221e\nft,B\u221e\nft)<L id(v\u221e\nlp,B0)with probability 1 (over the randomness of the training\nexamples).\nProof. Fine-tuning gets 0ID loss : It is well known from prior work (Laurent & von Brecht, 2018) that all\nlocal minima are global for optimizing two layer linear networks under convex losses (which is our setting),\nso if \ufb01ne-tuning converges to a local minimum, it actually converges to a global minimum of the train loss.\nSince there exists parameters that achieve 0loss on the training data (namely, B\u22c6,v\u22c6), this means \ufb01ne-tuning\ngets0loss on the training data as well. So for all training examples x(that is, rows of X):\nv\u221e\nft\u22a4B\u221e\nftx=w\u22a4\n\u22c6x. (A.197)\nSince the models are linear, this implies that \ufb01ne-tuning gets all examples in the span of the training examples\ncorrect as well. Since Pzhas density, and the number of training examples nis at least as large as the ID\nsubspace dimension m, the training examples span the ID subspace almost surely, so \ufb01ne-tuning gets every\nexample inx\u2208Scorrect almost surely, giving us:\nLid(v\u221e\nft,B\u221e\nft) = 0 (A.198)\n43",
    "Linear probing gets positive ID loss : Lemma A.19 shows that the ID error of linear probing is greater than\nzero under the same assumptions as this Proposition, so\nLid(v\u221e\nlp,B0)>0, (A.199)\nwhich \ufb01nishes the proof.\nWe now state and prove the Lemmas that we used to lower bound the ID error of linear probing.\nLemma A.17 gives conditions for when the projection F\u22a4wof a vectorwis not contained in the projection\nRange (F\u22a4E0)of the column space of a matrix E0.\nLemma A.17. Letw\u2208Rdbe a vector and F\u2208Rd\u00d7m,E0\u2208Rd\u00d7k,Eaug\u2208Rd\u00d7(k+1)have orthonormal\ncolumns, with Range (Eaug) =Span({w}\u222aRange (E0)). Ifm>k , we have:\nF\u22a4Eaugis full rank (A.200)\n(a)=\u21d2F\u22a4Eaughas higher rank than F\u22a4E0 (A.201)\n(b)\u21d0\u21d2F\u22a4w\u0338\u2208Range (F\u22a4E0) (A.202)\nProof. The proof of (a) is clear\u2014 F\u22a4Eaug\u2208Rm\u00d7(k+1)has rankk+ 1(since it is full rank and m\u2265k+ 1),\nbutF\u22a4Eaug\u2208Rm\u00d7khas rank at most kand is therefore lower rank. The assumption that m>k is crucial\nhere.\nFor (b), leta1,...,akbe the columns of E0, which form a basis for Range (E0). ThenF\u22a4a1,...,F\u22a4ak,F\u22a4w\nspans Range (F\u22a4Eaug), whileF\u22a4a1,...,F\u22a4akspans Range (F\u22a4E0). So (notice the \ufb01rst list of vectors\nhas an additional F\u22a4w) this means that dim( Range (F\u22a4Eaug))\u0338= dim( Range (F\u22a4E0))iffF\u22a4wis linearly\nindependent from the rest, that is, F\u22a4w\u0338\u2208Range (F\u22a4E0). Note that the rank of a matrix is the dimension of\nits range (column space), that is, dim( Range (A)) = rank(A)so this is what we wanted to show.\nThe next Lemma says that if the projection F\u22a4w\u22c6of the optimal linear model w\u22c6onto the ID subspace S, is\nnot contained in the projection Range (F\u22a4E0)of the features, then linear probing incurs non-zero ID error.\nLemma A.18. In the linear overparameterized setting, under the ID subspace assumption, if F\u22a4w\u22c6\u0338\u2208\nRange (F\u22a4E0), thenLid(v\u221e\nlp,B0)>0, whereE0\u2208Rd\u00d7kandF\u2208Rd\u00d7mhave orthonormal columns that\nform a basis for the feature rowspace R0=rowspace (B0)and ID subspace Srespectively.\nProof. We prove the contrapositive. Suppose Lid(v\u221e\nlp,B0) = 0 . This means that:\nLid(v\u221e\nlp,B0) =E\nx\u223cPid[(v\u22a4\n\u22c6B\u22c6x\u2212v\u221e\nlp\u22a4B0x)2] = 0 (A.203)\nSince the squared error is always non-negative, this means that v\u221e\nlp\u22a4B0x=w\u22a4\n\u22c6xalmost surely when x\u223cPid\n(recall that we de\ufb01ned w\u22c6=B\u22a4\n\u22c6v\u22c6). RecallPidis de\ufb01ned as: \ufb01rst pick z\u2208Pz(which has density) and then\noutputx=Fz. SincePzhas density, this implies that we get all examples in the ID subspace Scorrect:\nv\u221e\nlp\u22a4B0x=w\u22a4\n\u22c6xfor allx\u2208S. (A.204)\n44",
    "Since the columns of Fform an orthonormal basis for S, this gives us (since each column of Fis inS):\nv\u221e\nlp\u22a4B0F=w\u22a4\n\u22c6F. (A.205)\nNote that the rows of B0also form an orthonormal basis for R0just like the columns of E0. So we can\nchoosevwithv\u22a4E\u22a4\n0=v\u221e\nlp\u22a4B0. Then we have:\nv\u22a4E\u22a4\n0F=w\u22a4\n\u22c6F\u21d4F\u22a4E0v=F\u22a4w\u22c6 (A.206)\n\u21d4F\u22a4w\u22c6\u2208Range (F\u22a4E0), (A.207)\nwhere we took the transpose of both sides in the \ufb01rst step. This \ufb01nishes the proof of the contrapositive.\nFinally, Lemma A.19 combines Lemma A.17 and Lemma A.18 to give a more interpretable condition for the\nID error of linear probing: when the ID subspace Shas some components along the optimal linear model\nw\u22c6and the feature rowspace R0, then linear probing has non-zero error. This is measured in terms of the\nprincipal angle cos\u03b8max(Raug,S)between the ID subspace SandRaugwhich is the span of R0combined\nwithw\u22c6. This angle will typically be non-zero\u2014as an illustrative example, from Lemma A.16 we have that\nthis angle will be non-zero almost surely if the ID subspace Sis a uniformly random subspace.\nLemma A.19. In the linear overparameterized setting, under the ID subspace assumption, let R0=\nrowspace (B0), andRaug=Span({w\u22c6}\u222aR0). Ifw\u22c6\u0338\u2208R0andcos\u03b8max(Raug,S)>0, thenLid(v\u221e\nlp,B0)>\n0.\nProof. After a bit of setup, the proof simply combines Lemma A.17 and Lemma A.18. If w\u22c6\u0338\u2208R0, then\nRaughas dimension k+ 1. LetEaug\u2208Rd\u00d7(k+1),F\u2208Rd\u00d7mhave orthonormal columns which form a\nbasis forRaugandSrespectively. We assumed cos\u03b8max(Raug,S) =\u03c3min(F\u22a4Eaug)>0which means\nthatF\u22a4Eaugis full rank. The ID subspace assumption assumes that m > k . So from Lemma A.17,\nF\u22a4w\u22c6\u0338\u2208Range (F\u22a4E0)whereE0\u2208Rd\u00d7khas orthonormal columns that form a basis for R0. Then from\nLemma A.18, Lid(v\u221e\nlp,B0)>0.\nA.7 LP-FT\nWe start by showing a simple proposition, that if the initial feature extractor is perfect, then linear probing\nrecovers the optimal weights.\nProposition A.20. In the overparameterized linear setting, let R=rowspace (B0). IfB0=B\u22c6, and\ncos\u03b8max(S,R)>0, thenLood(v\u221e\nlp,B0) = 0 for allt.\nProof. We \ufb01rst show that because cos\u03b8max(R,S)>0, the training loss for linear probing is strongly convex.\nRecall that the training loss is:\n\u02c6L(v,B) =\u2225XB\u22a4v\u2212Y\u22252\n2 (A.208)\nLinear probing keeps B\ufb01xed asB0=B\u22c6and only tunes v, so we are interested in the Hessian of the loss\nwith respect to vevaluated at v,B\u22c6:\nHessv\u02c6L(v,B\u22c6) = 2(B\u22c6X\u22a4)(B\u22c6X\u22a4)\u22a4(A.209)\n45",
    "For strong convexity, it suf\ufb01ces to show that the min singular value of the Hessian is bounded away from 0 by\na constant. Recall the de\ufb01nition of cos\u03b8max(R,S). For someFwhose columns form an orthonormal basis\nforS, we have (since the rows of B\u22c6form an orthonormal basis for R):\n\u03c3k(B\u22c6F) = cos\u03b8max(R,S)>0 (A.210)\nNote thatB\u22c6Fis ak-by-nmatrix, so if the k-th singular value is positive it must be full rank. Since the\ncolumns ofX\u22a4spanF(since we de\ufb01ned Fto be such that the columns of Fare an orthonormal basis for S,\ni.e. the rows of X), this means B\u22c6X\u22a4is rankk. But that means the Hessian (B\u22c6X\u22a4)(B\u22c6X\u22a4)\u22a4is rankkas\nwell. So the linear probing loss is strongly convex.\nSince the loss is strongly convex, there is a unique minimizer, and gradient \ufb02ow converges to that. However,\nsince we are in the well-speci\ufb01ed setting, we know the training loss is:\n\u02c6L(v,B\u22c6) =\u2225XB\u22a4\n\u22c6v\u2212XB\u22a4\n\u22c6v\u22c6\u22252\n2 (A.211)\nSov=v\u22c6achieves 0 loss and must be the (unique) minimizer. Therefore we have shown that linear probing\nconverges to the unique minimizer v\u221e\nlp=v\u22c6, which attains 0 loss, as desired.\nNote that the entire proof works out if B0=UB\u22c6for some rotation matrix U. In that case, the Hessian\nbecomes 2U(B\u22c6X\u22a4)(B\u22c6X\u22a4)\u22a4U\u22a4which is still rank k, since multiplying by square rotation matrices does\nnot change the rank. In this case, the minimizer of the loss is v=Uv\u22c6, since (UB\u22c6)\u22a4(Uv\u22c6) =B\u22a4\n\u22c6v\u22c6. So\nlinear probing converges to v\u221e\nlp=Uv\u22c6, which achieves 0 loss, as desired.\nRestatement of Proposition 3.7. Suppose we have perfect pretrained features B0=UB\u22c6for some rotation\nU. LetR0=rowspace (B0). Under the non-degeneracy conditions cos\u03b8max(R0,S)\u0338= 0,cos\u03b8max(R0,S\u22a5)\u0338=\n0:\n\u2200t,L ood(Bft(t)\u22a4vft(t))>0,ifv0\u223cN(0,\u03c32I)is randomly initialized (FT) , (A.212)\n\u2200t,L ood(Bft(t)\u22a4vft(t)) = 0,ifv0is initialized to v\u221e\nlp(LP-FT). (A.213)\nProof. We \ufb01rst use Proposition A.20, which in the proof we showed still works if B0=UB\u22c6for some rotation\nmatrixU(which doesn\u2019t have to be identity). We get that v\u221e\nlp=Uv\u22c6. Then we have B\u22a4\n0v\u221e\nlp=B\u22a4\n\u22c6v\u22c6=w\u22c6.\nWe now just show that the gradients with respect to the training loss \u02c6Lat(v\u221e\nlp,B0)is 0, so gradient \ufb02ow does\nnot update the parameters at all.\nThe training loss is:\n\u02c6L(v,B) =\u2225XB\u22a4v\u2212XB\u22a4\n\u22c6v\u22c6\u22252\n2 (A.214)\nThe derivative with respect to vis:\n\u2202v\u02c6L(v,B) = 2BX\u22a4(XB\u22a4v\u2212XB\u22a4\n\u22c6v\u22c6) (A.215)\nThen sinceB\u22a4\n0v\u221e\nlp=B\u22a4\n\u22c6v\u22c6, we have:\n\u2202v\u02c6L(v\u221e\nlp,B0) = 0 (A.216)\nNext, the derivative with respect to Bis:\n\u2202B\u02c6L(v,B) = 2v(XB\u22a4v\u2212XB\u22a4\n\u22c6v\u22c6)\u22a4X (A.217)\n46",
    "Table 3: OOD accuracies with 90% con\ufb01dence intervals over 3 runs, for each of the three OOD domains in\nthe split of DomainNet used by Tan et al. (2020); Prabhu et al. (2021). LP does better than FT across the\nboard, and LP-FT does the best.\nReal Painting Clipart\nFine-tuning 55.29 (0.52) 50.26 (0.98) 60.93 (2.15)\nLinear probing 87.16 (0.18) 74.50 (0.58) 77.29 (0.12)\nLP-FT 86.82 (0.51) 75.91 (0.73) 79.48 (0.90)\nThen sinceB\u22a4\n0v\u221e\nlp=B\u22a4\n\u22c6v\u22c6, we have:\n\u2202B\u02c6L(v\u221e\nlp,B0) = 0 (A.218)\nSo since both the derivatives are 0, we have\u2202tvft(t) = 0 and\u2202BBft(t) = 0 , which means the parameters\ndon\u2019t change at all\u2014at all times twe havevft(t) =Uv\u22c6andBft(t) =UB\u22c6which gives us zero OOD loss:\nLood(Bft(t)\u22a4vft(t)) = 0 as desired.\nB More information on experiments\nIn this Appendix, we include more details on the datasets, pretraining methods, and adaptation methods. We\nalso include the OOD accuracies for \ufb01ne-tuning and linear-probing if we early stop and choose the learning\nrate based on OOD data, where we see that linear-probing is still typically better than \ufb01ne-tuning OOD.\nFinally, we include results for additional baselines, pretraining models, and conclude with a discussion about\nthe effective robustness of LP-FT.\nB.1 Dataset and method details\nWe use a diverse range of datasets and pretraining strategies.\n\u2022CIFAR-10\u2192STL: We \ufb01ne-tune or linear probe on CIFAR-10 (Krizhevsky, 2009) and test on\nSTL (Coates et al., 2011). This is a benchmark used in domain adaptation papers (French et al., 2018).\nCIFAR-10 and STL share 9 classes, so we follow the common practice of omitting the unshared class\nin STL (which is the \u2018monkey\u2019 class) when reporting accuracies. We use a publicly available MoCo-v2\nResNet-50 checkpoint pretrained on unlabeled examples from ImageNet-1k (Russakovsky et al., 2015),\nand \ufb01ne-tune for 20 epochs.\n\u2022DomainNet : We use the dataset splits in Tan et al. (2020) which is also used by follow-up work, e.g.,\nin Prabhu et al. (2021). This is different from the original version of the DomainNet dataset (Peng\net al., 2019), speci\ufb01cally Tan et al. (2020) note that some domains and classes contain many mislabeled\noutliers, so they select the 40 most common classes from the \u2018sketch\u2019, \u2018real\u2019, \u2018clipart\u2019 and \u2018painting\u2019\ndomains. We use the \u2018sketch\u2019 domain as ID, and all other domains (\u2018real\u2019, \u2018clipart\u2019, \u2018painting\u2019) as OOD,\nand in the main paper we report the average accuracies across the OOD domains. In Table 3 we see\nthat the same trends hold for each of the three OOD domains . We use a CLIP (Radford et al., 2021)\npretrained ResNet-50 model, and \ufb01ne-tune for 50 epochs (since this is a smaller dataset).\n47",
    "\u2022Living-17 andEntity-30 : We use a publicly available MoCo-v2 ResNet-50 checkpoint pretrained on\nunlabeled examples from ImageNet-1k (Russakovsky et al., 2015), and \ufb01ne-tune for 20 epochs. Note\nthat Living-17 and Entity-30 are subpopulation shifts derived from ImageNet, but the pretraining is\ndone on unlabeled data and does not see any OOD labels, following the pretraining and \ufb01ne-tuning\nstrategy in Cai et al. (2021). Entity-30 is a relatively large dataset that contains around 140K training\nexamples.\n\u2022FMoW Geo-shift : We adapt the version of the dataset from (Koh et al., 2021). We use training data\nfrom \u2018North America\u2019 to \ufb01ne-tune or linear probe, and then evaluate on validation data from Africa\nand Europe. We use a MoCo-TP (Ayush et al., 2020) checkpoint, pretrained on unlabeled FMoW\nsatellite images. We \ufb01ne-tune for 50 epochs here since the ID training dataset is smaller (around 20K\nexamples).\n\u2022CIFAR-10\u2192CIFAR-10.1 (Recht et al., 2018): We follow the same protocols as CIFAR-10 \u2192STL,\nexcept we test on CIFAR-10.1.\n\u2022ImageNet : we linear probe or \ufb01ne-tune on ImageNet (Russakovsky et al., 2015), and evaluate on\nImageNetV2 (Recht et al., 2019), ImageNet-R (Hendrycks et al., 2020), ImageNet-A (Hendrycks\net al., 2019b), and ImageNet-Sketch (Wang et al., 2019). We use a CLIP pretrained ViT-B/16 (vision\ntransformer), the largest publicly available CLIP model (Radford et al., 2021). We ran \ufb01ne-tuning for\n10 epochs, linear probing for 10 epochs. To equalize the runtime for LP-FT, we ran the linear probing\nstage for 5 epochs, and then the \ufb01ne-tuning stage for 5 epochs. We used a batch size of 128 for all\nmethods.\nTuning for ImageNet experiments. We swept over three learning rates for \ufb01ne-tuning (0.0001, 0.0003,\n0.001) and linear probing (0.01, 0.03, 0.1)\u2014as is standard we use larger learning rates for linear probing. For\nLP-FT, we swept over 3 learning rates (0.01, 0.03, 0.1) for the 5-epoch linear probing step. We took the run\nthat had the best ImageNet (ID) validation accuracy, and then swept over 3 learning rates (0.00001, 0.00003,\n0.0001) for the 5-epoch \ufb01ne-tuning step\u2014we use a lower learning rate for LP-FT since the experiments on\nthe other datasets suggested that the optimal learning rate that maximizes ID validation accuracy for LP-FT\nis smaller. We did not \ufb01nd the comparisons to be particularly sensitive to learning rate choice.\nAugmentations for ImageNet experiments. We used augmentations for \ufb01ne-tuning, and no augmentations\nfor linear probing, following Kornblith et al. (2019). This might raise a question of whether linear probing\nand LP-FT do better OOD because of the lack of augmentations. So as an ablation we also tried \ufb01ne-tuning\nwithout augmentations, however that led to worse accuracy (than \ufb01ne-tuning with augmentations) both ID\nand OOD. We now give details on the preprocessing and augmentations that we used. On ImageNet, for\nlinear probing and LP-FT, we used no augmentations\u2014we just resized each image so that the smaller side\nhas size 224 with bicubic interpolation, and then center-crop to a 224-by-224 image. For \ufb01ne-tuning, we used\naugmentations: speci\ufb01cally we use RandomResizedCrop in TorchVision, with the default arguments and\nsetting the size of the crop to 224, and then apply a random horizontal \ufb02ip.\nNotes on pretrained model choice. We note that our results say that the pretraining has to be good (e.g., at\nleast get reasonable accuracy ID) for linear probing to outperform \ufb01ne-tuning OOD. So, for example, we use\na model pretrained on unlabeled satellite images for the satellite image dataset\u2014if we pretrain the model on\nImageNet, we expect that \ufb01ne-tuning might do better. Similarly, for DomainNet we use a CLIP pre-trained\n48",
    "Table 4: OOD accuracies with 90% con\ufb01dence intervals over 3 runs, when \ufb01ne-tuning gets to choose\nlearning rate and early stop, and linear probing gets to choose \u21132regularization weights, on OOD data. We\nsee that linear probing still typically does better OOD (the only \ufb02ip from before is on FMoW).\nCIFAR-10.1 STL Ent-30 Liv-17 DomNet FMoW\nFT 92.27 (0.36) 85.97 (0.38) 64.09 (0.19) 78.63 (0.53) 59.43 (2.49) 40.23 (3.12)\nLP 82.67 (0.22) 86.53 (0.01) 69.15 (0.13) 82.39 (0.14) 79.91 (0.24) 37.12 (0.01)\nImNetV2 ImNet-R ImNet-Sk ImNet-A Average\nFT 71.5 (-) 52.4 (-) 40.5 (-) 27.8 (-) 61.3\nLP 69.7 (-) 70.9 (-) 46.4 (-) 46.1 (-) 67.1\nmodel, which is pretrained on the very large WebImageText dataset, and sees a variety of photo and sketch\nlike images. Pretraining on ImageNet alone does not lead to high accuracies on DomainNet (features are not\nvery good), so we do not necessarily expect linear probing to outperform \ufb01ne-tuning with these lower quality\nfeatures (for example, see the MoCo ablation in our main paper where we used a worse pretrained model,\nand \ufb01ne-tuning did better OOD).\nSanity check of \ufb01ne-tuning implementation. As a sanity check of our implementation, \ufb01ne-tuning did\nsubstantially better than training from scratch on all datasets (both ID and OOD) and matched existing\n\ufb01ne-tuning numbers where available (e.g. ResNet50 on CIFAR-10 (Chen et al., 2020b) and Entity-30 (Cai\net al., 2021)). Fine-tuning and linear probing also both do substantially better than training from scratch, ID\nand OOD, across the datasets. For example, on Living-17, training from scratch gets 89.3% ID and 58.2%\nOOD (Santurkar et al., 2020) which is over 5% worse ID and nearly 20% worse OOD, than all the adaptation\nmethods. For reference linear probing gets 96.5% ID and 82.2% OOD, and \ufb01ne-tuning gets 97.1% ID and\n77.8% OOD. This is even though training from scratch was run for 300 epochs, which is 15 times longer than\n\ufb01ne-tuning and LP-FT.\nB.2 Target early stopping\nIn the main paper, one ablation we mention is early stopping each \ufb01ne-tuning method and choose the best\nlearning rate based on target validation accuracy. As expected, \ufb01ne-tuning does improve a little, but linear\nprobing (average accuracy: 67.1%) is still better than \ufb01ne-tuning (average accuracy: 61.3%). Table 4 shows\nthe full results for all datasets.\nB.3 Feature change\nWe examine how much the features changed for ID and OOD examples in each dataset. Speci\ufb01cally, for each\ndataset, for each input example in the held out validation set, we computed the Euclidean distance of the\nResNet-50 features before and after \ufb01ne-tuning. We averaged these numbers across the dataset, showing the\nresults for ID validation examples in Table 5, and for OOD examples in Table 6.\nThe feature distortion theory predicts that the features for ID examples change more than for OOD examples.\nThis bears out in 9 out of 10 cases, that is all cases except for FT on FMoW. To see this, compare each cell in\nTable 5 with the corresponding cell in Table 6\u2014the former is higher in 9 out of 10 cases.\n49",
    "Table 5: In-distribution (ID) : Average distance that features move before and after \ufb01ne-tuning or LP-FT,\nmultiplied by 100 to make things easier to read. For linear probing the numbers are all 0, since the features\nare not tuned. As predicted by our theory, we see that features for ID examples (this table) move more than\nfeatures for OOD examples (Table 6). Both sets of features change substantially less for LP-FT. As usual we\nshow 90% con\ufb01dence intervals over three runs.\nCIFAR-10 Entity-30 Living-17 DomainNet FMoW\nFT 2.23 (0.03) 3.05 (0.02) 1.88 (0.01) 207.6 (12.31) 4.87 (0.15)\nLP-FT 0.07 (0.00) 0.03 (0.01) 0.11 (0.01) 0.19 (0.03) 0.57 (0.19)\nTable 6: Out-of-distribution (OOD) : Average distance that features move before and after \ufb01ne-tuning or\nLP-FT, multiplied by 100 to make things easier to read. For linear probing the numbers are all 0, since the\nfeatures are not tuned. As predicted by our theory, we see that features for ID examples (Table 5) move more\nthan features for OOD examples (this table). Both sets of features change substantially less for LP-FT. As\nusual we show 90% con\ufb01dence intervals over three runs.\nSTL Entity-30 Living-17 DomainNet FMoW\nFT 1.70 (0.04) 2.60 (0.02) 1.67 (0.01) 159.97 (16.23) 5.62 (0.30)\nLP-FT 0.04 (0.00) 0.02 (0.00) 0.09 (0.01) 0.18 (0.02) 0.54 (0.17)\nThe feature distortion theory says that this large feature change is caused because the head is randomly\ninitialized\u2014since the head needs to be updated by a large amount, the feature extractor is also updated a lot\nbecause the updates are coupled. Our theory predicts that if the head is initialized via linear probing then the\nfeature extractor should change a lot less for both ID and OOD examples. As predicted by the theory, across\nall the datasets in Table 5 and Table 6, the features change a lot less for LP-FT than for FT. For example, on\nCIFAR-10, the features change 30 \u00d7less for LP-FT than for FT.\nThese results suggest that \ufb01ne-tuning underperforms OOD, and LP-FT does well ID and OOD, for the reasons\npredicted by the feature distortion theory.\nB.4 Additional architectures, \ufb01ne-tuning methods\nThe main contributions of our paper are conceptual understanding and theory. However, to strengthen the\nempirical investigation we ran two additional models (a CLIP vision transformer and CLIP ResNet-50), as\nwell as three additional \ufb01ne-tuning heuristics. We focus on the Living-17 dataset because some of these\nablations require lots of compute and can take a long time to run on all the datasets.\nArchitectures and pretraining source : In the main paper, we showed results when initializing with a\nMoCo-v2 ResNet-50 model pretrained on unlabeled ImageNet examples. Here we examine how the results\nchange when we 1. Use a ResNet-50 model pretrained on CLIP\u2019s WebImageText dataset (Table 7), and,\n2. Use a much larger vision transformer model (ViT-B/16) pretrained on CLIP\u2019s WebImageText dataset\n(Table 8)\u2014this is the largest publicly available CLIP model at the time of writing. We see that similar \ufb01ndings\nto our main paper hold\u2014\ufb01ne-tuning does better than linear probing ID, but does worse than linear probing\n50",
    "Table 7: ID and OOD accuracies on Living-17 using a CLIP ResNet-50 model pretrained on the WebImage-\nText dataset, instead of unlabeled ImageNet examples. Similar \ufb01ndings hold\u2014here \ufb01ne-tuning does similarly\nto linear probing ID, but does worse than linear probing OOD. LP-FT does better than both ID, and closes\n86% of the gap OOD. As usual we show 90% con\ufb01dence intervals over three runs.\nID OOD\nLP 94.7 (0.2) 78.6 (0.5)\nFT 94.7 (0.1) 67.3 (0.8)\nLP-FT 95.6 (0.2) 77.0 (0.6)\nTable 8: ID and OOD accuracies on Living-17 using a CLIP ViT-B/16 (Vision Transformer) model pretrained\non the WebImageText dataset, instead of unlabeled ImageNet examples. This is the largest publicly available\nCLIP model that we could \ufb01nd. The same \ufb01ndings hold\u2014\ufb01ne-tuning does better than linear probing ID, but\ndoes worse than linear probing OOD. LP-FT does better than both ID, and closes 75% of the gap OOD. As\nusual we show 90% con\ufb01dence intervals over three runs.\nID OOD\nLP 97.5 (0.1) 87.6 (0.5)\nFT 97.8 (0.0) 81.5 (2.1)\nLP-FT 98.0 (0.0) 86.1 (0.1)\n(\u2018underperforms\u2019) OOD. Finally, LP-FT does better than both methods ID, and closes most (75%-90%) of\nthe gap OOD.\nThese results are from early stopping on ID validation data. If we early stop on OOD validation data, LP-FT\nachieves 87.9\u00b10.4% OOD accuracy, and LP gets 88.3\u00b10.2% OOD accuracy and here there is no statistically\nsigni\ufb01cant difference between the two. On the other hand, even if we early stop on OOD validation data,\n\ufb01ne-tuning gets 84.4\u00b10.5% OOD accuracy which is lower.\nFine-tuning heuristics : Transfer learning (initializing with a pretrained model, and then adapting it to a\ndownstream task) is the standard way to build modern ML models, because it improves accuracy and speeds\nup training. Since this paradigm is so widely used, there are many heuristics people use when training their\nmodels (as mentioned in the main paper, LP-FT has sometimes been used as a heuristic as well, although not\nin the context of OOD). We showed that LP-FT is one way to do well ID and OOD, but we hope that our\ntheory leads to even better \ufb01ne-tuning algorithms.\nIn this section, we compare LP-FT with additional \ufb01ne-tuning heuristics: using a larger learning rate for the\nhead layer, regularizing the features towards their original values, and side-tuning (Zhang et al., 2020) where\nwe freeze the features but add a side-network.\nThe intuitions from our theory suggest two other potential ways to improve OOD accuracy: 1. We could use\na higher learning rate on the linear layer, so that the linear layer learns quicker and the features do not get as\ndistorted, and 2. We could regularize the weights of the feature extractor towards the pretrained initialization,\nto prevent feature distortion. These heuristics have been used in prior work on \ufb01ne-tuning as well, for example\n51",
    "method 2 corresponds to L2-SP in (Li et al., 2018).\nWe run these two approaches on Living-17. For approach (1), we use a 10 \u00d7higher learning rate for the\nlinear layer, and for approach (2) we regularize the Euclidean distance between the current feature extractor\nweights (so ignoring the linear head) from the pretrained weights, multiplying by a hyperparameter \u03bb. We\ngrid search over the same learning rates as \ufb01ne-tuning for both methods, and in addition for (2) we grid\nsearch over \u03bb\u2208{1.0,0.1,0.01,0.001,0.0001}, so this amounts to sweeping over 30 hyperparameters as\nopposed to just 6 for \ufb01ne-tuning and LP-FT. For each hyperparameter con\ufb01guration we run 3replication runs\nwith different seeds to reduce the estimation variance, and early stop and model select using ID data just like\nfor \ufb01ne-tuning and LP-FT. Just like for \ufb01ne-tuning and LP-FT, we use a cosine learning rate decay and train\nfor the same number of epochs. Indeed, we \ufb01nd that both (1) and (2) are able to close part of the OOD gap\nbetween \ufb01ne-tuning and linear-probing. However, LP-FT does better than both methods ID and OOD. The\nfull results are in Table 9.\nWe also compare with another method, (3) side-tuning (Zhang et al., 2020). Side-tuning freezes the pretrained\nfeaturesg(x)but trains another \u2018side\u2019 model s(x), and then outputs v\u22a4(g(x) +h(x)), where the head vand\nthe parameters of the side model sare tuned. The intuition for trying this is that side-tuning also preserves\nthe pretrained features which likely reduces feature distortion. In the supplementary of Zhang et al. (2020)\nthey use a ResNet-50 for both the original model and the side model in their vision experiments, so we do\nthe same. We sweep over twelve learning rates ( 3\u00b710\u22125,1\u00b710\u22124,3\u00b710\u22124,..., 1.0,3.0,10.0), with three\nreplication runs with different seeds for each learning rate. Just like for \ufb01ne-tuning and LP-FT, we use a\ncosine learning rate decay and train for the same number of epochs, and we early stop and model select\nusing ID validation data. We checked that the best learning rate was not at the boundary of the grid search.\nOn OOD, side-tuning (81.0%) improves over \ufb01ne-tuning (77.7%). However, side-tuning doesn\u2019t do as well\nID. LP-FT did better ID and OOD. This could be because side-tuning does not get to re\ufb01ne the pretrained\nfeatures for the ID task\u2014while the side-network is powerful enough to learn good features, it is initialized\nrandomly and effectively trained from scratch, so it might not be able to learn these good features on the\nlimited sized training dataset (around 40K examples). The results are also in Table 9.\nWe also include results for training from scratch in Table 9\u2014these results are from Santurkar et al. (2020).\nNote that training from scratch was done for 450 epochs, whereas \ufb01ne-tuning was done for 20 epochs. As a\nsanity check, all the \ufb01ne-tuning methods and linear probing do substantially better than training from scratch,\nboth ID and OOD.\nB.5 Discussion of effective robustness\nLP-FT gets higher OOD accuracy than \ufb01ne-tuning, but it sometimes gets higher ID accuracy as well. Taori\net al. (2020) and Miller et al. (2021) show that OOD accuracy can often be correlated with ID accuracy, and\nsuggest examining the effective robustness: intuitively the extra gain in OOD accuracy than can be predicted\nfrom improved ID accuracy alone. Is LP-FT simply better in-distribution, or does it have higher effective\nrobustness as well?\nWe start out by noting that linear probing clearly has higher effective robustness in most of our datasets.\nLinear probing does worse than \ufb01ne-tuning ID so based on the effective robustness framework we would\nexpect it to do worse than \ufb01ne-tuning OOD as well. However, linear probing does better than \ufb01ne-tuning\nOOD and therefore has higher effective robustness.\n52",
    "Table 9: ID and OOD accuracies on Living-17 including three additional \ufb01ne-tuning heuristics, where we (1)\nUse a 10\u00d7larger learning rate for the head, or (2) Regularize the Euclidean distance of the feature extractor\nweights to the pretrained initialization, and (3) side-tuning where we freeze the pretrained model but add\na side network that is \ufb01ne-tuned. As a sanity check, all methods do better than training from scratch ID\nand OOD, and we show 90% con\ufb01dence intervals over three runs. As per the intuitions from the feature\ndistortion theory, these methods do mitigate feature distortion to some extent and improve OOD accuracy\nover \ufb01ne-tuning. LP-FT does better than all methods ID and OOD\u2014nonetheless, we believe that LP-FT is\njust the \ufb01rst step and hope that our theory can be used to inspire or derive better algorithms.\nID OOD\nScratch 92.4 (1.3) 58.2 (2.4)\nLP 96.5 (0.1) 82.2 (0.2)\nFT 97.1 (0.1) 77.7 (0.7)\nFT (10x Linear) 97.2 (0.2) 80.4 (0.3)\nFT (regularized) 97.1 (0.2) 80.0 (0.4)\nSide-tuning 95.5 (0.4) 81.0 (0.7)\nLP-FT 97.8 (0.1) 82.6 (0.3)\nThe solutions found by LP-FT also appear to have higher effective robustness than \ufb01ne-tuning, because when\nthey have similar ID accuracy, LP-FT does much better OOD. For a few pieces of evidence:\n1.On CIFAR-10\u2192STL, there is no statistically signi\ufb01cant difference between FT and LP-FT on ID, but\nLP-FT gets 8% higher accuracy OOD in Table 2.\n2.If we look at checkpoints earlier in training for CIFAR-10 \u2192STL we can exactly equalize ID accuracy\nand compare OOD accuracies. In-distribution, LP-FT and FT both get 97.2% accuracy, but OOD,\nLP-FT (90.2%) is much better than FT (81.8%).\n3.Finally, in Figure 3 we plot the OOD accuracy against the ID accuracy for \ufb01ne-tuning and LP-FT\non Living-17. We plot these for three different pretrained models (CLIP ResNet-50, CLIP ViT-B/16,\nMoCo-V2 ResNet-50). We see that the ID-OOD line for LP-FT is above the line for FT indicating\neffective robustness.\nNote that higher effective robustness does not mean a method is better. For example, a method A can have\nhigher effective robustness B by doing a lot worse in-distribution even when they have the same OOD\naccuracy. In this case, A is clearly inferior since it does worse ID and same OOD, but has higher effective\nrobustness because of its worse ID accuracy.\nWe believe the \ufb01nding that linear probing and LP-FT has higher effective robustness than \ufb01ne-tuning when\nthe distributon shift is large is particularly interesting because Taori et al. (2020) and Miller et al. (2021)\nshow that it is uncommon for methods to have higher effective robustness. In our case linear probing and\nLP-FT appear to consistently have higher effective robustness which suggests that with good transfer learning\nmethods we can get both high in-distribution accuracy and higher effective robustness.\n53",
    "Figure 3: We plot the OOD accuracy against ID accuracy on Living-17 for the three methods we consider,\nwhen we start from three different pretrained models (CLIP ResNet-50, CLIP ViT-B/16, MoCo-V2 ResNet-\n50). The line for linear probing and LP-FT lie above \ufb01ne-tuning which suggests that they have higher effective\nrobustness. Each point is produced by averaging over three random seeds.\nC Additional related work\nTheoretical analysis of overparameterized models. Modern deep learning presents an interesting paradigm\nfor theoretical analysis where the number of parameters is much larger than the number of training points.\nThe model class is highly expressive and several solutions obtain zero training loss even in the presence of\nnoise. Such overparameterized models have received a lot of interest recently especially with a focus on\nunderstanding \u201cbenign over\ufb01tting\u201d or the phenomenon where \ufb01tting noisy training data to zero loss leads to\nclassi\ufb01ers that generalize well. By analyzing different linear overparameterized settings Belkin et al. (2019);\nHastie et al. (2019); Bartlett et al. (2019); Muthukumar et al. (2020); Mei & Montanari (2019); Bibas et al.\n(2019) study various statistical properties such as the \u201cdouble descent curve\u201d in addition to benign over\ufb01tting.\nOne important aspect of overparameterized models is that there is no unique minimizer of the training loss.\nWe need some inductive bias which is typically implicit via the optimization procedure. Prior works study the\nstatistical properties of the explicit inductive bias of minimum norm interpolation. In contrast, we study the\neffect of gradient based optimization from a particular pretrained initialization where we effectively capture\nthe exact implicit inductive bias of gradient based \ufb01ne tuning.\n54\nFigure. The image depicts a graph with two lines, one of which is labeled \"D\" and the other \"A\". The line represents the number of samples that were used to train the model, while the second line represents the number of data points collected for each sample. There are several dots in the graph, representing different parts of the dataset.\n"
]